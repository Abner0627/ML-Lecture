<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Summary_Unsupervised Learning</title><style>
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	text-indent: -1.7em;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.highlight-default {
}
.highlight-gray {
	color: rgb(155,154,151);
}
.highlight-brown {
	color: rgb(100,71,58);
}
.highlight-orange {
	color: rgb(217,115,13);
}
.highlight-yellow {
	color: rgb(223,171,1);
}
.highlight-teal {
	color: rgb(15,123,108);
}
.highlight-blue {
	color: rgb(11,110,153);
}
.highlight-purple {
	color: rgb(105,64,165);
}
.highlight-pink {
	color: rgb(173,26,114);
}
.highlight-red {
	color: rgb(224,62,62);
}
.highlight-gray_background {
	background: rgb(235,236,237);
}
.highlight-brown_background {
	background: rgb(233,229,227);
}
.highlight-orange_background {
	background: rgb(250,235,221);
}
.highlight-yellow_background {
	background: rgb(251,243,219);
}
.highlight-teal_background {
	background: rgb(221,237,234);
}
.highlight-blue_background {
	background: rgb(221,235,241);
}
.highlight-purple_background {
	background: rgb(234,228,242);
}
.highlight-pink_background {
	background: rgb(244,223,235);
}
.highlight-red_background {
	background: rgb(251,228,228);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(55, 53, 47, 0.6);
	fill: rgba(55, 53, 47, 0.6);
}
.block-color-brown {
	color: rgb(100,71,58);
	fill: rgb(100,71,58);
}
.block-color-orange {
	color: rgb(217,115,13);
	fill: rgb(217,115,13);
}
.block-color-yellow {
	color: rgb(223,171,1);
	fill: rgb(223,171,1);
}
.block-color-teal {
	color: rgb(15,123,108);
	fill: rgb(15,123,108);
}
.block-color-blue {
	color: rgb(11,110,153);
	fill: rgb(11,110,153);
}
.block-color-purple {
	color: rgb(105,64,165);
	fill: rgb(105,64,165);
}
.block-color-pink {
	color: rgb(173,26,114);
	fill: rgb(173,26,114);
}
.block-color-red {
	color: rgb(224,62,62);
	fill: rgb(224,62,62);
}
.block-color-gray_background {
	background: rgb(235,236,237);
}
.block-color-brown_background {
	background: rgb(233,229,227);
}
.block-color-orange_background {
	background: rgb(250,235,221);
}
.block-color-yellow_background {
	background: rgb(251,243,219);
}
.block-color-teal_background {
	background: rgb(221,237,234);
}
.block-color-blue_background {
	background: rgb(221,235,241);
}
.block-color-purple_background {
	background: rgb(234,228,242);
}
.block-color-pink_background {
	background: rgb(244,223,235);
}
.block-color-red_background {
	background: rgb(251,228,228);
}
.select-value-color-default { background-color: rgba(206,205,202,0.5); }
.select-value-color-gray { background-color: rgba(155,154,151, 0.4); }
.select-value-color-brown { background-color: rgba(140,46,0,0.2); }
.select-value-color-orange { background-color: rgba(245,93,0,0.2); }
.select-value-color-yellow { background-color: rgba(233,168,0,0.2); }
.select-value-color-green { background-color: rgba(0,135,107,0.2); }
.select-value-color-blue { background-color: rgba(0,120,223,0.2); }
.select-value-color-purple { background-color: rgba(103,36,222,0.2); }
.select-value-color-pink { background-color: rgba(221,0,129,0.2); }
.select-value-color-red { background-color: rgba(255,0,26,0.2); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="dd4113cf-0564-4179-b40a-fb7309dd7dbd" class="page sans"><header><h1 class="page-title">Summary_Unsupervised Learning</h1></header><div class="page-body"><nav id="eb8e17b7-e938-4263-b96d-900fe0abf820" class="block-color-gray table_of_contents"><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#35a0efe7-d05e-401d-9353-827c3dff6c76">Unsupervised Learning</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#789b86a4-bf1f-40bc-b1ae-2fc97d141f37">Dimension Reduction</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#31beec4c-dd52-4c8f-a935-903e5d665f66">一、Clustering</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#d7e5abb6-985c-4da0-b3db-272af904d560">1. K-means</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#f027b8d3-8fae-43c3-b474-fc27eb21c2a3">2. Hierarchical Agglomerative Clustering (HAC)</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#4420b466-8cfb-46dc-81a4-b37c5f69f2a7">二、Distributed Representation (Dimension Reduction)</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#64903f68-122e-49d8-bd7a-579df184f9a4">1. Feature Selection</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#8e154491-aa76-4321-bcc4-8400bcbec995">2. Principle Component Analysis (PCA)</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#61892ea7-2f33-4b77-b03e-cc4d871d2a3d">(1) Lagrange Multiplier</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#e568a443-8a1a-4d30-8d57-c23d50755e6c">(2) SVD</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#6c6cbb93-4b0b-4e38-b0d6-a3ff0fb8b321">(3) Neural Network</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#37fe1be7-9d07-4332-b362-0b2473f10824">Word Embedding</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#19ea5cbb-3a83-43f8-9341-3835cb9c8d4c">一、1-of-N Encoding</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#5eb109ad-7756-4fc9-afbd-714bb66aef9e">二、Word Class</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#147c7365-972f-4b98-ac8e-1db6a019b3f1">三、Word Embedding</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#125d8463-dcd8-4dad-ba37-5970d68affb0">1. Count Based</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#ebcb180c-01eb-4521-bb5f-c64c30bf2916">2. Perdition based</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#35f07fda-bddb-43fb-81f4-7209c2f0cf32">(1) Sharing Parameters</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#a50f6dde-9b8b-4890-8e31-3c0573a44b55">(2) Training（以下截自影片字幕檔）</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#942e10c9-7f48-4ee2-8a5d-86dd9cf40f9f">(3) Continuous bag of word (CBOW)</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#ee432ebf-a10c-4942-92d7-d596afa28037">(4) Skip-gram</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#360f94e5-5ee9-4ac3-afbe-3fd3a0fbff29">Neighbor Embedding</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#1b94be68-cfa2-41f2-a649-9c38ab4b8506">一、Locally Linear Embedding (LLE)</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#43be7523-e9d6-4ea5-b524-ad91cfa761e2">二、Laplacian Eigenmaps</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#ea1a4e20-ec4d-4fb9-b5e9-76038d23e39d">三、T-distributed Stochastic Neighbor Embedding (t-SNE)</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#7fcb2e13-4fec-45b0-8e36-df74c17bd244">1. t-SNE–Similarity Measure</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#9b85d2a2-9d4c-4de1-8e9a-879c034e8c53">Auto-encoder</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#0de310e9-3abc-44e3-95e4-6328fe2483c9">一、Deep Auto-encoder</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#3f604cfb-d0eb-4767-8edc-95e0abf6f132">二、Applications</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#17a21c4f-6fe0-40c8-84a3-8bc1de3bd635">1. Encoder for Text Retrieval</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#dce1be8d-857a-453f-8f4a-fe73057251f6">2. Encoder for Similar Image Search</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#bf61ff49-48f2-4624-9484-a12d95ba140f">3. Auto-encoder for CNN</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#645705d6-b38a-49f4-a96d-2bc846c3bfc9">(1) Unpooling</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#58c4da1d-9868-4cc5-9460-8428a345b857">(2) Deconvolution</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#1a83a104-b9e2-45e0-bb6a-5e0d26aabc17">4. Auto-encoder for Pre-training DNN</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#8790c8ec-e89d-4554-b056-d336bc56dceb">5. Decoder for Drawing</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#b9e57dc4-26c9-4035-9e13-e25f70ede628">Generative Models</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#8d39afd5-c3e3-42a5-bd15-9e20d50a0204">一、Component-by-component</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#9d868be8-7642-454a-9bdd-2907c68a6280">二、Variational Auto-encoder (VAE)</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#124e9da4-18bd-4049-8e9a-a97a7b5edd85">三、Generative Adversarial Network (GAN)</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#6ad4a63f-9ede-4b0d-9061-b21217f4b910">1. Discriminator</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#0facbdaa-e49d-43d5-9cf1-f0776f5fa150">2. Generator</a></div></nav><h1 id="35a0efe7-d05e-401d-9353-827c3dff6c76" class="">Unsupervised Learning</h1><p id="cb6dd568-ccee-4e6c-be87-1bcce394855a" class="">Data皆不具label，且訓練時僅有input而無法直接獲得output的學習模式。
主要可分作兩類，Dimension Reduction與Generation。</p><h1 id="789b86a4-bf1f-40bc-b1ae-2fc97d141f37" class="">Dimension Reduction</h1><p id="0512f3a4-2b2d-43a2-ac2b-5f0486a5242c" class="">基本精神為「化簡為繁」，意即把本來比較複雜的input變成比較簡單的output。</p><figure id="3bdeae6c-95ef-428f-a307-18e0abe5dec0" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image1.png"><img style="width:502px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image1.png"/></a></figure><h2 id="31beec4c-dd52-4c8f-a935-903e5d665f66" class="">一、Clustering</h2><p id="069fccc8-e392-4e11-9880-8539a15202b7" class="">假設現在要做image的clustering，那就是把一大堆的image分成好幾類。
將本來有些不同的image都用同一個class來表示。</p><figure id="aceec997-0163-4a6e-adce-9db1bf83f88d" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/Untitled.png"><img style="width:627px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/Untitled.png"/></a></figure><h3 id="d7e5abb6-985c-4da0-b3db-272af904d560" class="">1. K-means</h3><p id="bc223d3a-92be-489d-be8d-d66bd2e0011e" class="">將一大堆的unlabeled data把他們分作K個cluster。
首先就是找這些cluster的center，從training data裡面隨機找K個object出來，
當成K個cluster的center。</p><p id="15052fb8-92cb-4428-9f39-a1ebc876540e" class="">接下來決定每一個object屬於1到K的哪一個cluster。
假設現在的object xn，跟第i個cluster的center最接近的話，那xn就屬於ci。</p><p id="68b7b204-e23a-4ffe-a6e0-f379545ff903" class="">簡而言之用一個binary 的value b（上標n，下標i）來代表第n個object有沒有屬於第i個class，
如果第n個object屬於第i個class的話，那這一個binary的value就是1，反之就是0。</p><p id="cb1ac049-9cc1-4d2b-9cda-8c4e6f659941" class="">接下來，就是update cluster，把所有屬於第i個cluster的object做平均，
得到第i個cluster的center ci。</p><p id="c30be4f5-bcb1-43c8-87ba-2d17963147bf" class="">最後就重複上述步驟即可。</p><figure id="faddc0e8-eb82-44af-9423-d18465c1686a" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/Untitled%201.png"><img style="width:680px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/Untitled%201.png"/></a></figure><h3 id="f027b8d3-8fae-43c3-b474-fc27eb21c2a3" class="">2. Hierarchical Agglomerative Clustering (HAC)</h3><p id="8d5f439e-f10c-49a7-a188-dc88560ab112" class="">該方法是先建一個tree，假設現在有5個example，兩兩去算他的相似度，
然後挑最相似的那一個pair出來。</p><p id="a79c7555-0be2-4721-a306-7826a0c3e403" class="">假設現在最相似的pair，是第一個和第二個example，
那就把第一個example和第二個example merge起來，
像是對他們取平均得到一個新的vector（下圖黃色方塊），同時代表第一個和第二個example。</p><p id="88ba7f11-1710-44ce-9158-c3c0322645f8" class="">接下來變成有四個example，再對這4筆data兩兩去計算他們的相似度，
假設是第三筆和第四筆最像，那就再把他們merge起來，得到另外一筆data（下圖紅色方塊）。</p><p id="d6b62f8b-c784-4f3f-b764-636e6abf1a31" class="">最終得到這個tree的root，建立出一個tree structure，
並決定在這個tree structure上面哪地方切一刀，將example分成好幾個cluster。</p><figure id="215f94e2-a53f-4289-8b1b-ca9e1e4fd9e0" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image4.png"><img style="width:521px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image4.png"/></a></figure><p id="dd301b3f-612b-4eae-883b-ab9ae3d65fbf" class="">HAC跟K-means最大的差別就是，如何決定cluster的數目。
在K-means裡面要需要決定那個K的value是多少，而到底有多少個cluster是不容易決定的；</p><p id="6ac5ea83-b526-4412-a981-a45644f565be" class="">HAC的好處就是不直接決定幾個cluster，而是決定要切在這個樹的structure的哪裡。</p><h2 id="4420b466-8cfb-46dc-81a4-b37c5f69f2a7" class="">二、Distributed Representation (Dimension Reduction)</h2><p id="47ef2880-0c70-4aba-906b-54f39c7ccaf5" class="">然而在做cluster的時候比較以偏概全，因為每一個object最後都必須要屬於某一個cluster。
實際上來說應該用一個vector來表示各個object，
那這個vector裡面的每一個dimension就代表了某一種attribute。</p><p id="3df4db42-e32c-4c2b-974e-93021b2b09d7" class="">該方式就稱distributed representation。</p><h3 id="64903f68-122e-49d8-bd7a-579df184f9a4" class="">1. Feature Selection</h3><p id="9cccca5c-b923-4d32-bb9f-b15803b05997" class="">假設data的分布本來在二維的平面上，然後發現幾乎都集中在x2的dimension 而已，
如此就可以拿掉x1這個dimension。</p><p id="d42d0089-f607-44c1-950c-9af53f5e02ce" class="">然而這個方法不見得總是有用，因為有很多時候處理的case是任何一個 dimension都不能拿掉的。</p><figure id="f620cba7-2c12-43b5-9dd7-6f7c0f2ae852" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image5.png"><img style="width:138px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image5.png"/></a></figure><h3 id="8e154491-aa76-4321-bcc4-8400bcbec995" class="">2. Principle Component Analysis (PCA)</h3><p id="94fd21f3-36ef-4bca-bf15-a2f6a1a20d9c" class="">假設這個function是一個很簡單的linear function，且input x跟這個output z之間的關係為線性的
（linear transform），也就是把這個x乘上一個matrix W可得到output z。</p><p id="01cfb18b-839e-4cd5-b142-01b55368f500" class="">那現在要做的事情就是根據一大堆的x把W找出來。
可理解成將x投影到W上，使他們在W有較大的variance，而投影後在W上的點就是z。</p><figure id="609d64aa-342f-4b49-ad7b-7a781ede0935" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image6.png"><img style="width:398px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image6.png"/></a></figure><p id="85097b3f-c992-49e2-8093-5e58bb3014f2" class="">假設把x投影到一維，我們希望選一個w1，他經過projection以後，得到的這些 z1的分布越大越好。也就是說，我們不希望通過這個projection以後，所有的點通通擠在一起（見上圖）。</p><p id="173f403c-3278-4b85-a6bc-743e31b44cc8" class="">所以我們希望找一個projection的方向，它可以讓projection後的variance 越大越好。
因此現在要去maximize的對象是z1的variance，就是summation over所有的(z1 - z1\bar) 的平方；
而z1\bar就是做z1的平均值，因此只要找到一個w1讓z1的variance最大就結束了。</p><p id="9d5ed7cf-37ce-4dd9-bded-91fe315d2834" class="">再來可能不只要投影到一維，推廣至二維的情況大略相同。同樣是找一個w2讓z2的variance最大。
只是在一維時我們必須限制w1的2-norm，使w1跟x做內積能直接得到z1；
但是為避免w2與w1同值，必須再限制兩者的內積值為1。</p><figure id="07b16bcb-1fd2-4f15-8d63-75c4112bba2c" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/Untitled%202.png"><img style="width:495px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/Untitled%202.png"/></a></figure><figure id="b3ac4c84-ad37-40e4-9b62-caca260484e1" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image8.png"><img style="width:589px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image8.png"/></a></figure><h3 id="61892ea7-2f33-4b77-b03e-cc4d871d2a3d" class="">(1) Lagrange Multiplier</h3><p id="c30173e7-1e32-4f39-b687-f0e8f694df22" class="">前面提到z1等於w1跟x的內積值，那z1的平均值就是summation over所有 w1跟x的內積再除以總數。可進一步簡化成w1與x\bar的內積，即：</p><figure id="e5647892-7cdd-4d7b-80b7-63927a791f98" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><msub><mi>z</mi><mn>1</mn></msub><mo>ˉ</mo></mover><mo>=</mo><msup><mi>w</mi><mn>1</mn></msup><mo>⋅</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><mo>∑</mo><mi>x</mi><mo>=</mo><msup><mi>w</mi><mn>1</mn></msup><mo>⋅</mo><mover accent="true"><mi>x</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{z_1}=w^1\cdot\frac{1}{N}\sum x=w^1\cdot\bar{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.71778em;vertical-align:-0.15em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.56778em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord">ˉ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8641079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.00744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-symbol large-op" style="position:relative;top:-0.000004999999999977245em;">∑</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8641079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.56778em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.56778em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">ˉ</span></span></span></span></span></span></span></span></span></span></span></div></figure><p id="1167ece3-21cc-4740-86cd-0e61364e9bd5" class="">而z1的variance可整理為w1的transpose乘上x的covariance再乘上w1。</p><p id="da9c7e3a-2b85-42cd-9b2b-2bd4bfba231a" class="">所以現在要解的問題是找出一個w1可以maximize該式，但optimization 的對象是有constraint的。
如果沒有constraint的話，這裡的w1每一個值都變無窮大就結束了，
所以這裡的constraint 是說w1的2-norm要等於1。</p><figure id="9e01fba0-8d9c-41d5-a33b-bfbbe1be1e2f" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image9.png"><img style="width:606px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image9.png"/></a></figure><p id="38aac649-62a0-4938-a69c-db2ff803f58a" class="">此處用S來描述x的covariance matrix。
由於S是symmetric又是positive-semidefinite的關係，他所有的eigenvalue都是non-negative的。</p><p id="5f009186-d042-45eb-a36d-a2527d68d045" class="">接著用Lagrange multiplier（開頭如下式假設），</p><figure id="756bfb52-628b-4b0a-9fee-c5bcd9f745f8" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo stretchy="false">(</mo><msup><mi>w</mi><mn>1</mn></msup><mo stretchy="false">)</mo><mtext> </mtext><mo>=</mo><mtext> </mtext><mo stretchy="false">(</mo><msup><mi>w</mi><mn>1</mn></msup><msup><mo stretchy="false">)</mo><mi>T</mi></msup><mi>S</mi><msup><mi>w</mi><mn>1</mn></msup><mtext> − </mtext><mi>α</mi><mo stretchy="false">(</mo><mo stretchy="false">(</mo><msup><mi>w</mi><mn>1</mn></msup><msup><mo stretchy="false">)</mo><mi>T</mi></msup><msup><mi>w</mi><mn>1</mn></msup><mtext> − </mtext><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">g(w^1) = (w^1)^TSw^1 − α((w^1)^Tw^1 − 1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"> </span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1413309999999999em;vertical-align:-0.25em;"></span><span class="mord"> </span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mord"> </span><span class="mord">−</span><span class="mord"> </span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="mopen">(</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mord"> </span><span class="mord">−</span><span class="mord"> </span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span></div></figure><p id="03578a2a-a989-49f2-9300-72f914d7c21d" class="">把這個function對w的第一個element做偏微分，再對第二個element做偏微分，依此類推。
然後令這些式子通通等於0，整理完後會得到一個式子帶入w1後使其為0。</p><figure id="760e4c09-84f4-4a32-9d8f-72d006faa6fa" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo stretchy="false">(</mo><msup><mi>w</mi><mn>1</mn></msup><mo stretchy="false">)</mo><mtext> − </mtext><mi>α</mi><mo stretchy="false">(</mo><msup><mi>w</mi><mn>1</mn></msup><mo stretchy="false">)</mo><mtext> </mtext><mo>=</mo><mtext> </mtext><mn>0</mn></mrow><annotation encoding="application/x-tex">S(w^1) − α(w^1) = 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"> </span><span class="mord">−</span><span class="mord"> </span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"> </span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord"> </span><span class="mord">0</span></span></span></span></span></div></figure><p id="472be0df-2078-4a1a-ab87-cb08b47b6963" class="">而w1就是S的eigenvector，接下來看哪一個eigenvector代到下式，可以maximize該式。</p><figure id="64bf6c70-e8ee-4417-86f6-e2ccbf926f0f" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>w</mi><mn>1</mn></msup><msup><mo stretchy="false">)</mo><mi>T</mi></msup><mi>S</mi><mo stretchy="false">(</mo><msup><mi>w</mi><mn>1</mn></msup><mo stretchy="false">)</mo><mtext> </mtext><mo>=</mo><mtext> </mtext><mi>α</mi><mo stretchy="false">(</mo><msup><mi>w</mi><mn>1</mn></msup><msup><mo stretchy="false">)</mo><mi>T</mi></msup><mo stretchy="false">(</mo><msup><mi>w</mi><mn>1</mn></msup><mo stretchy="false">)</mo><mtext> </mtext><mo>=</mo><mtext> </mtext><mi>α</mi></mrow><annotation encoding="application/x-tex">(w^1)^TS(w^1) = α(w^1)^T(w^1) = α</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1413309999999999em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"> </span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1413309999999999em;vertical-align:-0.25em;"></span><span class="mord"> </span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"> </span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord"> </span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span></span></span></span></span></div></figure><p id="8bf77ae9-ab78-459e-91f2-42d5a0a69172" class="">所以問題變成找一個w1使α最大。
而當α最大時，這個α就是最大的 eigenvalues λ1；w1是對應到最大的eigenvalue的eigenvector。</p><figure id="016c31a4-7f57-4e1a-b05a-3d9692ff7067" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image10.png"><img style="width:593px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image10.png"/></a></figure><p id="d051d31a-8618-4e9e-806d-3637042d18a3" class="">同理，如果要找w2的話，就要maximize根據w2投影以後的variance：</p><figure id="47b4428d-54b6-47ed-87be-99f124d7448e" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>w</mi><mn>2</mn></msup><msup><mo stretchy="false">)</mo><mi>T</mi></msup><mi>S</mi><mo stretchy="false">(</mo><msup><mi>w</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(w^2)^TS(w^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1413309999999999em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></div></figure><p id="5b2eee46-6d6a-431d-9f45-f251a0cf99d5" class="">同樣假設function g 裡面包含了你要 maximize 的對象，還有兩個constraint
（w1跟w2他們是orthogonal的），然後分別乘上α跟β。</p><figure id="f6d27125-6392-4b55-9961-b23dde2e7d66" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo stretchy="false">(</mo><msup><mi>w</mi><mn>2</mn></msup><mo stretchy="false">)</mo><mtext> </mtext><mo>=</mo><mtext> </mtext><mo stretchy="false">(</mo><msup><mi>w</mi><mn>2</mn></msup><msup><mo stretchy="false">)</mo><mi>T</mi></msup><mi>S</mi><mo stretchy="false">(</mo><msup><mi>w</mi><mn>2</mn></msup><mo stretchy="false">)</mo><mtext> − </mtext><mi>α</mi><mo stretchy="false">(</mo><mo stretchy="false">(</mo><msup><mi>w</mi><mn>2</mn></msup><msup><mo stretchy="false">)</mo><mi>T</mi></msup><msup><mi>w</mi><mn>2</mn></msup><mtext>−</mtext><mn>1</mn><mo stretchy="false">)</mo><mtext> − </mtext><mi>β</mi><mo stretchy="false">(</mo><mo stretchy="false">(</mo><msup><mi>w</mi><mn>2</mn></msup><msup><mo stretchy="false">)</mo><mi>T</mi></msup><msup><mi>w</mi><mn>1</mn></msup><mtext> − </mtext><mn>0</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">g(w^2) = (w^2)^TS(w^2) − α((w^2)^Tw^2−1) − β((w^2)^Tw^1 − 0)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"> </span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1413309999999999em;vertical-align:-0.25em;"></span><span class="mord"> </span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"> </span><span class="mord">−</span><span class="mord"> </span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="mopen">(</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord">−</span><span class="mord">1</span><span class="mclose">)</span><span class="mord"> </span><span class="mord">−</span><span class="mord"> </span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span><span class="mopen">(</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mord"> </span><span class="mord">−</span><span class="mord"> </span><span class="mord">0</span><span class="mclose">)</span></span></span></span></span></div></figure><p id="7ba59694-a865-49f3-b926-d0f3dbef1121" class="">接下來對所有的參數做偏微分得到這個值：</p><figure id="9c94e585-fc95-48e5-b8a2-d63e118468af" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo stretchy="false">(</mo><msup><mi>w</mi><mn>2</mn></msup><mo stretchy="false">)</mo><mtext> − </mtext><mi>α</mi><mo stretchy="false">(</mo><msup><mi>w</mi><mn>2</mn></msup><mo stretchy="false">)</mo><mtext> − </mtext><mi>β</mi><mo stretchy="false">(</mo><msup><mi>w</mi><mn>1</mn></msup><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">S(w^2) − α(w^2) − β(w^1)= 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"> </span><span class="mord">−</span><span class="mord"> </span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"> </span><span class="mord">−</span><span class="mord"> </span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span></span></div></figure><p id="5e18cd91-dd4a-4dff-a9c2-029ed9246c9d" class="">接著式子左邊同乘w1的transpose變為：</p><figure id="3365ecca-2d3a-4548-a625-eb0a8ca09cda" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>w</mi><mn>1</mn></msup><msup><mo stretchy="false">)</mo><mi>T</mi></msup><mi>S</mi><mo stretchy="false">(</mo><msup><mi>w</mi><mn>2</mn></msup><mo stretchy="false">)</mo><mtext> − </mtext><mi>α</mi><mo stretchy="false">(</mo><msup><mi>w</mi><mn>1</mn></msup><msup><mo stretchy="false">)</mo><mi>T</mi></msup><mo stretchy="false">(</mo><msup><mi>w</mi><mn>2</mn></msup><mo stretchy="false">)</mo><mtext> − </mtext><mi>β</mi><mo stretchy="false">(</mo><msup><mi>w</mi><mn>1</mn></msup><msup><mo stretchy="false">)</mo><mi>T</mi></msup><mo stretchy="false">(</mo><msup><mi>w</mi><mn>1</mn></msup><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">(w^1)^TS(w^2) − α(w^1)^T(w^2) − β(w^1)^T(w^1)= 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1413309999999999em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"> </span><span class="mord">−</span><span class="mord"> </span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"> </span><span class="mord">−</span><span class="mord"> </span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span></span></div></figure><p id="4b556468-a380-418a-b3ad-c7ebf4935e61" class="">紅字部分為一個scalar (vector* matrix* vector)，而scalar在做transpose以後還是他自己，
所以transpose結果是一樣的，得到：</p><figure id="40080a1d-d4a9-4926-ad66-c6a933c8f729" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>w</mi><mn>1</mn></msup><msup><mo stretchy="false">)</mo><mi>T</mi></msup><mi>S</mi><mo stretchy="false">(</mo><msup><mi>w</mi><mn>2</mn></msup><mo stretchy="false">)</mo><mtext> </mtext><mo>=</mo><mtext> </mtext><mo stretchy="false">(</mo><msup><mi>w</mi><mn>2</mn></msup><msup><mo stretchy="false">)</mo><mi>T</mi></msup><mo stretchy="false">(</mo><msup><mi>S</mi><mi>T</mi></msup><mo stretchy="false">)</mo><mo stretchy="false">(</mo><msup><mi>w</mi><mn>1</mn></msup><mo stretchy="false">)</mo><mtext> </mtext><mo>=</mo><mtext> </mtext><mo stretchy="false">(</mo><msup><mi>w</mi><mn>2</mn></msup><msup><mo stretchy="false">)</mo><mi>T</mi></msup><mi>S</mi><mo stretchy="false">(</mo><msup><mi>w</mi><mn>1</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(w^1)^TS(w^2) = (w^2)^T(S^T)(w^1) = (w^2)^TS(w^1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1413309999999999em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"> </span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1413309999999999em;vertical-align:-0.25em;"></span><span class="mord"> </span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"> </span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1413309999999999em;vertical-align:-0.25em;"></span><span class="mord"> </span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></div></figure><p id="d70f08ca-06d0-4268-a077-e5a2de3d4150" class="">（因為S是symmetric的，所以transpose以後還是他自己）</p><p id="92483b69-5182-4f0b-b624-797ba84efeb2" class="">接下來我們已經知道w1是S的eigenvector，而且它對應到最大的eigenvalue λ1，所以寫為下式：</p><figure id="a75ea257-93c7-406f-930e-6e0d13bc74ff" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∵</mo><mi>S</mi><mo stretchy="false">(</mo><msup><mi>w</mi><mn>1</mn></msup><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><msup><mi>λ</mi><mn>1</mn></msup><mo stretchy="false">)</mo><mo stretchy="false">(</mo><msup><mi>w</mi><mn>1</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">∵S(w^1)= (λ^1)(w^1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69224em;vertical-align:0em;"></span><span class="mrel amsrm">∵</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">λ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></div></figure><figure id="430d150d-9dad-4022-9060-906b50ac965f" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∴</mo><mtext> </mtext><mo stretchy="false">(</mo><msup><mi>w</mi><mn>2</mn></msup><msup><mo stretchy="false">)</mo><mi>T</mi></msup><mi>S</mi><mo stretchy="false">(</mo><msup><mi>w</mi><mn>1</mn></msup><mo stretchy="false">)</mo><mtext> </mtext><mo>=</mo><mtext> </mtext><mo stretchy="false">(</mo><msup><mi>w</mi><mn>2</mn></msup><msup><mo stretchy="false">)</mo><mi>T</mi></msup><mo stretchy="false">(</mo><msup><mi>λ</mi><mn>1</mn></msup><mo stretchy="false">)</mo><mo stretchy="false">(</mo><msup><mi>w</mi><mn>1</mn></msup><mo stretchy="false">)</mo><mtext> </mtext><mo>=</mo><mtext> </mtext><mo stretchy="false">(</mo><msup><mi>λ</mi><mn>1</mn></msup><mo stretchy="false">)</mo><mo stretchy="false">(</mo><msup><mi>w</mi><mn>1</mn></msup><mo stretchy="false">)</mo><mo stretchy="false">(</mo><msup><mi>w</mi><mn>2</mn></msup><msup><mo stretchy="false">)</mo><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">∴ (w^2)^TS(w^1) = (w^2)^T(λ^1)(w^1) = (λ^1)(w^1)(w^2)^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69224em;vertical-align:0em;"></span><span class="mrel amsrm">∴</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1413309999999999em;vertical-align:-0.25em;"></span><span class="mord"> </span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"> </span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1413309999999999em;vertical-align:-0.25em;"></span><span class="mord"> </span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">λ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"> </span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1413309999999999em;vertical-align:-0.25em;"></span><span class="mord"> </span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">λ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span></div></figure><p id="2633e5d8-6263-410b-aba8-e4ccf35ff365" class="">因為(w1)*(w2)T 又等於0 (orthogonal)，所以得到的結論是如果β等於0的話，
剩下的S*(w2)會等於α*(w2)。</p><p id="0f135c27-825b-436e-9892-8b4b548c9907" class="">因此w2也是一個eigenvector且必須跟w1 orthogonal，故選第二大的w2，
然後它對應到第二大的eigenvalue λ2。
其餘維度依此類推。</p><figure id="6c6af6ec-4c0e-4b2f-bd32-a6b8e355b4d9" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image11.png"><img style="width:606px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image11.png"/></a></figure><p id="275346b0-1a78-4326-be25-2e9e2578218d" class="">另外PCA中z的covariance會是一個diagonal matrix。</p><p id="f9e13e66-162d-48c5-b194-c9b5ae8c845e" class="">也就是說，假設PCA所得到的新的feature z給其他的model描述某一個class 的distribution 
（假設為generative model）。</p><p id="3b8c5bef-e326-4bd8-ae08-28456aaf94b9" class="">在做這個Gaussian的時候，會假設input data的covariance就是 diagonal，
且不同的dimension之間沒有correlation，這樣一來減少參數量。
所以它就可以用比較簡單的model來處理input data，避免overfitting的情形</p><figure id="f685aa37-44a7-4f99-bd9b-a06c6e9e64ea" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image12.png"><img style="width:611px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image12.png"/></a></figure><h3 id="e568a443-8a1a-4d30-8d57-c23d50755e6c" class="">(2) SVD</h3><p id="3c1ea4b1-527d-4f42-a54c-0d67882de6e0" class="">假設在考慮的是MNIST，這些數字其實是由一些basic的component（筆畫）所組成的。
那這些component寫作u1, u2, u3等等。</p><p id="50273511-6bcb-432d-bed9-4a32dff2ee7f" class="">則input x會等於u1這個 component乘上c1加上u2這個component乘上c2，以此類推。
然後再加上 x\bar代表所有的image的平均。</p><p id="0e048756-7da4-4b73-bc92-5494701365c9" class="">所以每一張image就是有一堆component的linear combination，然後再加上它的平均所組成的。</p><figure id="9c914239-5c34-4306-a9e0-5b7643afd556" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image13.png"><img style="width:600px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image13.png"/></a></figure><p id="0a56f27d-918d-45d3-9799-01e695abcbfb" class="">接著這一些linear combination的結果減去x\bar，該值必須與目標值x\head越近越好，即：</p><figure id="c5f043dd-29c0-4c4a-87a9-db6be758d7de" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>−</mo><mover accent="true"><mi>x</mi><mo>ˉ</mo></mover><mo>≈</mo><msub><mi>c</mi><mn>1</mn></msub><msup><mi>u</mi><mn>1</mn></msup><mo>+</mo><msub><mi>c</mi><mn>2</mn></msub><msup><mi>u</mi><mn>2</mn></msup><mo>+</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo>+</mo><msub><mi>c</mi><mi>K</mi></msub><msup><mi>u</mi><mi>K</mi></msup><mo>=</mo><mover accent="true"><mi>x</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">x-\bar{x}\approx c_1u^1+c_2u^2+...+c_Ku^K=\hat{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.56778em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.56778em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.0141079999999998em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">u</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.0141079999999998em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">u</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.0413309999999998em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">u</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">^</span></span></span></span></span></span></span></span></span></span></span></div></figure><p id="39904d76-ab6c-47b3-ad5b-f2783c7c038a" class="">因此就必須找K個vector去minimize他們的距離 (reconstruction error)</p><figure id="e634903c-20d2-486d-8174-334c73f179a1" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mi>e</mi><mi>c</mi><mi>o</mi><mi>n</mi><mi>s</mi><mi>t</mi><mi>r</mi><mi>u</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>e</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>r</mi><mtext>：</mtext><mo>∥</mo><mo stretchy="false">(</mo><mi>x</mi><mo>−</mo><mover accent="true"><mi>x</mi><mo>ˉ</mo></mover><mo stretchy="false">)</mo><mo>−</mo><mover accent="true"><mi>x</mi><mo>^</mo></mover><msub><mo>∥</mo><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">Reconstruction error：\parallel(x-\bar{x})-\hat{x}\parallel_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mord mathdefault">e</span><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mord mathdefault">s</span><span class="mord mathdefault">t</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">u</span><span class="mord mathdefault">c</span><span class="mord mathdefault">t</span><span class="mord mathdefault">i</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord cjk_fallback">：</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∥</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.56778em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel"><span class="mrel">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></div></figure><figure id="64d1357d-6e00-4e81-8626-61d02c06c9da" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><munder><mo><mi mathvariant="normal">arg min</mi><mo>⁡</mo></mo><mrow><mo stretchy="false">{</mo><msup><mi>u</mi><mn>1</mn></msup><mo separator="true">,</mo><mtext> </mtext><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mtext> </mtext><msup><mi>u</mi><mi>K</mi></msup><mo stretchy="false">}</mo></mrow></munder><mo>∑</mo><mo>∥</mo><mo stretchy="false">(</mo><mi>x</mi><mo>−</mo><mover accent="true"><mi>x</mi><mo>ˉ</mo></mover><mo stretchy="false">)</mo><mo>−</mo><mover accent="true"><mi>x</mi><mo>^</mo></mover><msub><mo>∥</mo><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">L=\argmin_{\{u^1,\ ...,\ u^K\}}\sum\parallel(x-\bar{x})-\hat{x}\parallel_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">L</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.2273050000000003em;vertical-align:-1.177305em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.66786em;"><span style="top:-2.097695em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">{</span><span class="mord mtight"><span class="mord mathdefault mtight">u</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7463142857142857em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mpunct mtight">,</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mtight">.</span><span class="mord mtight">.</span><span class="mord mtight">.</span><span class="mpunct mtight">,</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mtight"><span class="mord mathdefault mtight">u</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7740928571428571em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span></span></span></span></span></span></span></span><span class="mclose mtight">}</span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop"><span class="mord mathrm">a</span><span class="mord mathrm">r</span><span class="mord mathrm" style="margin-right:0.01389em;">g</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathrm">m</span><span class="mord mathrm">i</span><span class="mord mathrm">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.177305em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-symbol large-op" style="position:relative;top:-0.000004999999999977245em;">∑</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∥</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.56778em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel"><span class="mrel">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></div></figure><figure id="e3f98614-e252-4a14-a9f9-e1f0c14ea81a" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover><mo>=</mo><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msub><mi>c</mi><mi>k</mi></msub><msup><mi>u</mi><mi>k</mi></msup></mrow><annotation encoding="application/x-tex">\hat{x}=\sum^K_{k=1}c_ku^k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.1304490000000005em;vertical-align:-1.302113em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283360000000002em;"><span style="top:-1.8478869999999998em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.300005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.302113em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">u</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span></span></span></span></span></div></figure><p id="e91ffef7-0931-430e-bbf0-27f4b0ff949e" class="">接下來，可進一步將reconstruction error表示為matrix的乘積。</p><figure id="55bafd22-47df-4b62-bdb9-b4537b9b003e" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image14.png"><img style="width:545px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image14.png"/></a></figure><p id="cc203405-07c1-429f-a5c4-575ddb81a37d" class="">接著可以用SVD把matrix X拆成U，Σ與V三個matrix的乘積，U就是代表matrix uK；
Σ*V就是代表matrix cK。</p><p id="a007c2cb-1509-4ab8-8d7b-ced05e0c0414" class="">然後U這個matrix，他的k個column，其實就是一組orthonormal vector，
到的就是X*XT最大的k個的eigenvector。</p><p id="5138d4d5-0776-4804-9d52-86366a70609f" class="">而這個X*(XT)就是covariance matrix，也就是PCA找出來的那一些w
（covariance matrix的eigenvector）等同於解出來的U的每一個column的vector。</p><p id="df4e6eaa-3373-4d70-ae8a-592e1605a832" class="">換句話說，根據PCA找出來的那些w其實就是在minimize這個reconstruction error；
那Dimension Reduction的結果就是這些vector。</p><h3 id="6c6cbb93-4b0b-4e38-b0d6-a3ff0fb8b321" class="">(3) Neural Network</h3><p id="56978222-7ffc-4f76-a81c-b0215ef8f8c2" class="">已知從用PCA找出來的w1到wK就是K個component，u1, u2到uK，
再根據 component linear combination得到的結果叫做x\head，
也就是(wK)*ck做linear combination的結果。</p><p id="b2bfda03-6390-44c2-9b3b-2b340f45fa88" class="">接著我們會希望這個x\head跟(x - x\bar)他的距離越近越好，
也就是要minimize 這個reconstruction error。</p><p id="70f93e9a-040b-487d-97c4-04e2999d8964" class="">由於W已經找出來了，所以接下來只需要找的ck的值。由於這些K個vector wK是orthonormal 的，
因此只要把(x-x\bar)跟wk做內積，就可找出ck。</p><p id="ecf092a8-937e-4dd4-b0d9-7e0e90a4b5f8" class="">而該內積的過程可以想成用neural network來表示，
最終我們要使NN的output與(x - x\bar)的距離越近越好；</p><p id="84814e02-0c82-4f48-bd04-2dff3b77b57b" class="">換句話說就是讓input等於output，而這個東西就叫作Autoencoder。</p><figure id="4398d21a-a1b7-48c7-ab4c-191c834d425c" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image15.png"><img style="width:622px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image15.png"/></a></figure><h1 id="37fe1be7-9d07-4332-b362-0b2473f10824" class="">Word Embedding</h1><h2 id="19ea5cbb-3a83-43f8-9341-3835cb9c8d4c" class="">一、1-of-N Encoding</h2><p id="40cb31a5-dece-401a-b47c-424e9f269919" class="">將文字描述為vector的一種方式，這個vector的dimension，就是這個世界上可能有的詞彙數目，
而每一個文字，皆對應到其中一維。</p><h2 id="5eb109ad-7756-4fc9-afbd-714bb66aef9e" class="">二、Word Class</h2><p id="8e8b0c75-72ef-4684-8204-f3bcb1be8fde" class="">然而上述方式無法從vector獲得任何資訊，因此可進一步使用Word Class將同性質的文字歸做一類。等同於在做Dimension Reduction的時候做clustering的概念。</p><h2 id="147c7365-972f-4b98-ac8e-1db6a019b3f1" class="">三、Word Embedding</h2><p id="36d4c720-c45b-4d40-a501-5dcd97321bdf" class="">考量到光用一個feature無法將所有文字完全分開；或是class之間的相似程度無法區別，
例如下圖class 1與class 3雖然分別是動物及植物，但是他們皆屬生物比起class 2的相近度較高。</p><p id="ce5c2b11-fc36-4ba6-bdcb-8b54daa4cbf3" class="">使用Word Embedding的時候，就是把每一個word都project到一個high dimensional的space上面，而project後的vector又稱feature vector。</p><figure id="61883a8c-a757-4274-8855-d2e48eeb95d9" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image16.png"><img style="width:618px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image16.png"/></a></figure><h3 id="125d8463-dcd8-4dad-ba37-5970d68affb0" class="">1. Count Based</h3><p id="f6d517f6-f261-4715-8d4f-947581b7d44b" class="">如果現在有兩個詞彙，wi與wj常常在同一個文章中出現，
那他們的word vector就分別用V(wi)以及V(wj)來代表。
而這種方法有一個很代表性的例子，叫做Glove vector。</p><p id="8c9e5a7b-6f48-47f8-adf6-5e822fb2690e" class="">這個方法的原則是計算V(wi)以及V(wj)的內積；
而假設Nij是wi跟wj他們co-occur在同樣的document裡面的次數。
最後我們希望找一組wi與wj的vector使其內積與Nij越近越好。</p><figure id="6a9db9c3-7c0c-4693-80ce-462d4941d161" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image17.png"><img style="width:544px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image17.png"/></a></figure><h3 id="ebcb180c-01eb-4521-bb5f-c64c30bf2916" class="">2. Perdition based</h3><p id="dcff4567-afd0-40c0-a07e-a00eaf7396d1" class="">Prediction based的方法主要是learn一個neural network，
他做的事情是given 前一個word然後predict下一個可能出現的word是什麼。</p><p id="4345bcfe-5484-4568-9e12-7f2111565a7c" class="">假設給一個sentence，這邊的每一個w代表一個word；
這個neural network的input wi-1就是1-of-N encoding的vector；
output就是下一個word wi是某一個 word的機率，也就是說output的dimension就是vector的size，
假設現在世界上有10萬個word，這個model的output就是10萬維。</p><p id="7c43dbd0-effe-4424-ba1c-48b7f98c6ea8" class="">至於將input feature vector丟進去NN的時候，他會通過很多hidden layer。
接下來把第一個hidden layer的input拿出來，寫作他的第一個dimension是Z1，
第二個dimension是Z2，以此類推。
這一個input 1-of-N encoding得到Z的這個 vector就可代表一個word的embedding。</p><figure id="ba3da21a-a0b9-4458-8968-94d31a385c38" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image18.png"><img style="width:608px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image18.png"/></a></figure><h3 id="35f07fda-bddb-43fb-81f4-7209c2f0cf32" class="">(1) Sharing Parameters</h3><p id="66b3b83b-c058-4a47-87bc-34bae38d80e3" class="">如果只看一個詞彙，他下一個連接的詞彙有非常多種組合可能性。因此可以拓展這個問題，
希望machine learn的是input前面兩個詞彙wi-2跟wi-1並且predict下一個word wi。</p><p id="e78af983-c1a5-4672-9b10-804e419905af" class="">可以輕易地把這個model拓展到N個詞彙。一般而言如果要learn這樣的word vector，
至少需要10個詞彙才能夠learn出比較reasonable的結果。</p><p id="bc29b69c-9336-460e-86af-62309e7646f6" class="">這邊用input兩個word當作例子，值得注意的是一般的neural network，
就把 input wi-2跟wi-1的1-of-N encoding的vector接在一起變成一個很長的vector。</p><p id="fb90c1d3-70ae-47d7-a747-ab2da7973305" class="">接著直接丟到neural network裡面當作input就可以了。</p><p id="86fe907d-7ecd-4cc1-926a-e099441a4fa7" class="">但實際上會希望wi-2相連的weight跟wi-1相連的weight是被tight在一起的。
意思就是wi-2的第一個dimension跟第一個hidden layer的第一個neuron中間連的weight；
以及wi-1的第一個dimension跟第一個hidden layer的第一個neuron，他們之間連的weight。
這兩個weight必須是一樣的，以此類推。</p><p id="4ed986df-03ea-43b4-9b41-5dd787b366b1" class="">如果不這麼做的話，把同一個word放在wi-2的位置跟放在wi-1的位置，
通過這個transform以後得到的embedding就會不一樣；
另外一個理由在於可以減少參數量，因為input這個dimension很大，
所以就算這個feature vector是50維，也是一個非常大的matrix。
如果強迫讓所有的1-of-N encoding，他後面接的weight是一樣的，那就不會隨著contest的增長，
而需要這個更多的參數。</p><figure id="15401fb8-f2fb-4f88-83bd-47d23c2c4121" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image19.png"><img style="width:606px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image19.png"/></a></figure><p id="386d6f2a-4fd2-4a6b-ac89-d60f43941347" class="">進一步用formulation來表示該想法。假設wi-2的1-of-N encoding就是Xi-2；
wi-1的1-of-N encoding就是X i-1，那他們的長度都是V的絕對值。
這個 hidden layer的input寫做一個vector Z，長度為Z 的絕對值。</p><p id="7159fa2e-dd8a-4711-b001-3f318be027e2" class="">而Z等於Xi-2 * W1 + Xi-1 * W2。
現在這個W1跟W2都是一個Z乘上一個V dimension的weight matrix，
接著我們強制讓W1跟W2相等，等於一個一模一樣的matrix W。</p><p id="e059a2fe-03d3-4470-b529-bd9e935ad9d2" class="">也就是說在處理這個問題的時候，可以把Xi-2跟Xi-1直接先加起來，
再乘上W的這個transform就會得到z。</p><figure id="ba40d0ca-8028-49f0-8cc6-27c3ed63d348" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image20.png"><img style="width:623px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image20.png"/></a></figure><p id="0005efdc-d70e-40fa-a3cf-002414cfbb53" class="">事實上在train CNN的時候也有讓某一些參數必須是相同的需求。因此採用相同作法，假設我們希望wi跟wj他們的weight是一樣的，因此要給他們一樣的 initialization。</p><p id="875f91c8-09c7-40ea-929d-9d5af64da39c" class="">接下來計算wi對cost function的偏微分，然後update wi；
同理計算wj對cost function的偏微分，然後update wj。</p><figure id="d6220b56-7db0-4b82-b21d-065942a042f4" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image21.png"><img style="width:700px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image21.png"/></a></figure><p id="9c9df528-83de-4b6a-a166-560225e9294b" class="">然而如果他們對C的偏微分是不一樣的，那就必須把wi進一步減掉wj對C的偏微分；
同時把wj減掉wi對C的偏微分。也就是確保wi跟wj在訓練的過程中，
他們的 weight永遠都是被tight 在一起的。</p><figure id="9bdf7afd-2901-4063-ad47-8671ea7d92a5" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image22.png"><img style="width:700px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image22.png"/></a></figure><h3 id="a50f6dde-9b8b-4890-8e31-3c0573a44b55" class="">(2) Training（以下截自影片字幕檔）</h3><p id="a3eea49e-8c29-4da3-97b4-fc2899a0384d" class="">那要怎麼訓練這個network呢？這個network的訓練，完全是unsupervised的。
也就是說，你只要collect一大堆文字的data，然後接下來就可以train你的model。</p><p id="eb656658-14ce-462a-8bce-92438af23afa" class="">比如說這邊有一個句子就是：潮水退了，就知道誰沒穿褲子。
那就讓你的neural network input &quot;潮水&quot; 跟 &quot;退了&quot;，希望他的output是 &quot;就&quot; 。</p><p id="5bb32fec-c365-4b8b-95eb-8a6629f269e3" class="">所以你希望network的output跟 &quot;就&quot; 的1-of-N encoding，是minimize cross entropy。
然後再來就input &quot;退了 &quot; 跟 &quot;就&quot;，希望他的output跟 &quot;知道&quot; 越接近越好。</p><p id="ffe5d70c-e6af-489f-bd3c-267cb4d387ae" class="">最後output &quot;就&quot; 跟 &quot;知道&quot;，希望他跟 &quot;誰&quot; 越接近越好。</p><figure id="75d7567e-a109-48a6-ac0a-c33008972c20" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image23.png"><img style="width:631px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image23.png"/></a></figure><h3 id="942e10c9-7f48-4ee2-8a5d-86dd9cf40f9f" class="">(3) Continuous bag of word (CBOW)</h3><p id="04043a06-8906-4277-bc74-e8d6c0d7f332" class="">拿某一個詞彙的context去predict中間這個詞彙，也就是拿Wi-1跟Wi+1去 predict Wi。</p><figure id="8f43067d-773f-452b-9305-439493cf1b46" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image24.png"><img style="width:617px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image24.png"/></a></figure><h3 id="ee432ebf-a10c-4942-92d7-d596afa28037" class="">(4) Skip-gram</h3><p id="3267e129-b7ca-4bd4-b1f7-9291e83b862a" class="">拿中間的詞彙去predict接下來的context，也就是拿Wi去predict Wi-1跟Wi+1。</p><figure id="6493d4e2-1142-4440-8fc1-ce71ad38ce2d" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image25.png"><img style="width:619px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image25.png"/></a></figure><p id="e9a8c413-d136-4597-970e-9ce5dc26779d" class="">此外，上述Perdition based的各種變形，皆不需要deep就能做得起來，因此可以大幅減少運算量。</p><h1 id="360f94e5-5ee9-4ac3-afbe-3fd3a0fbff29" class="">Neighbor Embedding</h1><p id="08da6b94-c82e-40de-9cea-078340f459eb" class="">與前述的Dimension Reduction不同，雖然皆是處理降維的問題，
但Neighbor Embedding可以處理Manifold形式的data point。
由於在該例中兩點距離很遠的時候Euclidean distance不一定會成立，
因此PCA無法有效分析兩點的距離。</p><figure id="dc95b80c-cecd-4d08-bc23-e78b96b21165" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image26.png"><img style="width:264px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image26.png"/></a></figure><h2 id="1b94be68-cfa2-41f2-a649-9c38ab4b8506" class="">一、Locally Linear Embedding (LLE)</h2><p id="45ad2593-d6ec-45b8-a486-8bf347058855" class="">在原來的空間裡面有某一個點，xi，接著選出這個xi的neighbor xj。
接下來要找xi跟xj的關係，寫作wij。</p><p id="0066451e-9e03-43fd-9024-8102ca467252" class="">假設每一個xi，都可以用他的neighbor做linear combination，而wij就是拿xj去組合xi的weight。
我們希望這組wij對xi的所有neighbor xj做weighted sum的時候，他可以跟xi越接近越好。
也就是xi減掉summation over所有的wij乘以xj，他的two norm是越小越好的。</p><p id="27eebd3d-626e-432e-8606-4886a7659af3" class="">然後做dimension reduction把原來所有的xi跟xj轉成zi和zj，但是他們中間的關係wij，是不變的。</p><figure id="9f467f70-79f2-4616-84d0-8b7d3d562f61" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/Untitled%203.png"><img style="width:546px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/Untitled%203.png"/></a></figure><p id="2949e808-453c-4740-b3ad-6e12be8ffbf1" class="">原來這些xj可以做linear combination產生xi，而這些zj也可以用同樣的linear combination產生zi。
所以現在在這個式子裡面wij變成是已知的，接著要找一組z，讓zj透過wij做weighted sum以後，
他可以跟zi越接近越好。</p><p id="5c9952d9-da0c-4c2f-8611-e3249f32cd7b" class="">此外該方法中，neighbor的數量K選太小的時候，無法考慮距離較遠的點的情況；
至於K太大時，則會納入一些transform後關係太弱的neighbor。</p><figure id="3dd764ef-1591-46e2-9537-1b230392d3b5" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image28.png"><img style="width:508px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image28.png"/></a></figure><h2 id="43be7523-e9d6-4ea5-b524-ad91cfa761e2" class="">二、Laplacian Eigenmaps</h2><p id="7bab9e67-8691-4334-9de6-edacbeadfbf0" class="">然而比較兩點之間的距離，只算Euclidean distance是不足夠的，
要看的是他們在這個high density的region 之間的distance。
如果兩個點之間有high density的connection，才算是真正的接近。</p><p id="f06d6a49-3a42-4375-a633-93d20f58a0f8" class="">這件事情可以用一個graph來描述這件事情，也就是把data point construct成一個graph，
計算data point兩兩之間的相似度，如果相似度超過threshold，就把他們connect起來。</p><p id="77886a64-00cf-4bd5-93c2-fa756f7e8cb3" class="">此處可考慮smoothness的距離來建立graph。如果x1跟x2在high density的 region是close的，
那我們就會希望，z1跟z2也是相近的。</p><p id="c3c0c9db-28c4-4a31-9edd-e856854817b2" class="">上述描述可用smoothness的式子寫出來，解法就同semi-supervised learning：</p><figure id="b59bcb8b-3c51-435c-b42e-eb2f5e5c1bad" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><munder><mo>∑</mo><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></munder><msub><mi>w</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><msqrt><mrow><mo stretchy="false">(</mo><msup><mi>z</mi><mi>i</mi></msup><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>+</mo><mo stretchy="false">(</mo><msup><mi>z</mi><mi>j</mi></msup><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow></msqrt><mtext> </mtext><mo stretchy="false">(</mo><mn>2</mn><mo>−</mo><mi>n</mi><mi>r</mi><mi>o</mi><mi>m</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">S=\frac{1}{2}\sum_{i,j}w_{i,j}\sqrt{(z^i)^2+(z^j)^2}\  (2-nrom)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.735217em;vertical-align:-1.413777em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8723309999999997em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.413777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.984207em;"><span class="svg-align" style="top:-3.2em;"><span class="pstrut" style="height:3.2em;"></span><span class="mord" style="padding-left:1em;"><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.750664em;"><span style="top:-2.9890000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.740108em;"><span style="top:-2.9890000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.750664em;"><span style="top:-2.9890000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.740108em;"><span style="top:-2.9890000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span><span style="top:-2.9442070000000005em;"><span class="pstrut" style="height:3.2em;"></span><span class="hide-tail" style="min-width:1.02em;height:1.28em;"><svg width='400em' height='1.28em' viewBox='0 0 400000 1296' preserveAspectRatio='xMinYMin slice'><path d='M263,681c0.7,0,18,39.7,52,119
c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120
c340,-704.7,510.7,-1060.3,512,-1067
l0 -0
c4.7,-7.3,11,-11,19,-11
H40000v40H1012.3
s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232
c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1
s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26
c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z
M1001 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.25579299999999994em;"><span></span></span></span></span></span><span class="mspace"> </span><span class="mopen">(</span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">n</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">o</span><span class="mord mathdefault">m</span><span class="mclose">)</span></span></span></span></span></div></figure><p id="adf7c48e-2bd0-422d-8c92-cf252f269afb" class="">而wi,j代表如果今天兩個data point在圖上是相連的，那wi,j就是他們的相似程度；
若不相連，即為0。最後找出zi跟zj minimize S。</p><p id="bd26d495-322b-4499-a6bd-2cff16465a25" class="">(Review: semi-supervised learning)</p><figure id="6481df3a-60db-4ad2-a034-ec78c9823b90" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image29.png"><img style="width:601px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image29.png"/></a></figure><p id="d0e56b37-dab1-4214-8b62-4b40e21cd905" class="">然而為防止zi跟zj為相同的值，必須再加上constrain。
如果z降維以後的空間是M維的空間則希望z1到zN做 span 以後會等於RM，
也就是說z會佔據整個 M 維的空間。</p><p id="0a7619f0-ff87-4e02-a840-cce8846f0e3e" class="">而z與前述的graph Laplacian L是有關係的，他其實就是graph Laplacian中，
對應到比較小的eigenvalue的那些eigenvector。</p><figure id="329d8dc3-3b9c-42dc-8e3b-a7d1a8a3cf7e" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image30.png"><img style="width:552px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image30.png"/></a></figure><p id="69026aee-7cc7-4cac-996c-084c525b652d" class="">至於如果先找出z之後，再用K-means做clustering稱spectral clustering。</p><h2 id="ea1a4e20-ec4d-4fb9-b5e9-76038d23e39d" class="">三、T-distributed Stochastic Neighbor Embedding (t-SNE)</h2><p id="d4a38518-adcc-46e2-b266-4bf36c2e3114" class="">前述的方法有一個最大的問題就是，他只假設相近的點應該要是接近的，
但沒有假設不相近的點要盡量分開。</p><p id="462b1220-93bf-41d5-96a3-116f620f1df9" class="">比如用LLE在MNIST上時，他確實會把同個class的點都聚集在一起，
但沒有防止不同class的點不要疊成一團。</p><p id="1b661116-2bb2-456c-8724-000b2c6061fe" class="">而做t-SNE時一樣是做降維，把原來的data point x變成比較低維的vector z。
那在原來的x的space上計算所有的點的pair，xi和xj之間的similarity，寫作S(xi, xj)。</p><p id="8d1025a1-fd70-422f-bd19-13f5d5ba5bed" class="">接下來做normalization，計算P( xj | xi)。在分子的地方是xi跟xj的similarity，
然後分母的地方就是summation over除了xi以外，所有其他的點和xi之間所算出來的距離。</p><figure id="b80c5827-7e11-467d-a6e6-c84a46417ae2" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msup><mi>x</mi><mi>j</mi></msup><mo>∣</mo><msup><mi>x</mi><mi>i</mi></msup><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>S</mi><mo stretchy="false">(</mo><msup><mi>x</mi><mi>i</mi></msup><mo separator="true">,</mo><msup><mi>x</mi><mi>j</mi></msup><mo stretchy="false">)</mo></mrow><mrow><munder><mo>∑</mo><mrow><mi>k</mi><mo mathvariant="normal">≠</mo><mi>i</mi></mrow></munder><mi>S</mi><mo stretchy="false">(</mo><msup><mi>x</mi><mi>i</mi></msup><mo separator="true">,</mo><msup><mi>x</mi><mi>k</mi></msup><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">P(x^j\mid x^i)=\frac{S(x^i,x^j)}{\sum_{k\neq i}S(x^i,x^k)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.124664em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.874664em;"><span style="top:-3.1130000000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1246639999999999em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8746639999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.623482em;vertical-align:-1.1218180000000002em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5016639999999999em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.18639799999999984em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mrel mtight"><span class="mrel mtight"><span class="mord mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-2.7em;"><span class="pstrut" style="height:2.7em;"></span><span class="rlap mtight"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="inner"><span class="mrel mtight"></span></span><span class="fix"></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span></span><span class="mrel mtight">=</span></span><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.43581800000000004em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.750664em;"><span style="top:-2.9890000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7751079999999999em;"><span style="top:-2.9890000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.1218180000000002em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div></figure><p id="77076450-9e06-48a0-806a-e557c691a0f4" class="">另外假設已經找出了一個low dimension的representation zi跟zj，同樣也可以計算similarity S&#x27;。</p><p id="2aba543a-23a5-4824-9a94-94b0cde4b6c2" class="">同理可定義Q( zj | zi)，他的分子的地方就是S&#x27;( zi, zj)，
分母的地方就是 summation over zi跟所有database裡面的data point zk之間的距離。</p><p id="7f2faea4-44a0-4b6c-84dc-4e78089c3cf4" class="">此處做normalization是必要的，因為不知道在高維空間中算出來的距離S( xi, xj)跟S’( zi, zj)，
他們的scale是不是一樣的。</p><p id="06f10bd3-d17a-4cf8-8d11-4dfaf9fee4d2" class="">如果有做normalization，就可以把他們都變成機率，此時他們的值都會介於0到1之間，
其scale會是一樣的。</p><figure id="b1897ccb-779e-4ba5-a6a8-724c18349230" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image31.png"><img style="width:195px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image31.png"/></a></figure><figure id="6f9e68a1-78fd-43c4-8681-cc0967dab27b" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image32.png"><img style="width:557px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image32.png"/></a></figure><p id="83fc815e-232e-4871-9d1e-1f8063aa5ee2" class="">接下來我們希望找一組zi跟zj，讓這兩個distribution P與Q越接近越好。</p><p id="82f25b51-e50c-4c10-8088-49f18202a05c" class="">此處便是用KL divergence衡量兩個distribution之間的相似度，
也就是使這兩個distribution之間的KL divergence越小越好，
最後summation over所有的data point minimize L (gradient descent)。</p><figure id="f9f18e7a-bc59-424d-af18-7084be67239e" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image33.png"><img style="width:557px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image33.png"/></a></figure><p id="9a9232fa-497c-4ba8-ad91-b068dc5d4c1b" class="">此外在做t-SNE的時候，他會計算所有data point之間的similarity，所以其運算量有點大。
因此常見的做法是先用比較快的方法做降維 (PCA)，最後再使用t-SNE。</p><p id="323cdfb6-3d82-4dcd-8f96-c6fd046300a6" class="">然而如果給t-SNE一個新的data point他會無法處理，他只能夠先給他一大堆 x，
再把每一個x的z都找出來，接著繼續給他一個新的x。
所以該方法會需要重新跑一遍這一整套演算法，過程會變得相當麻煩。</p><p id="f30ba5a3-0f38-4abd-b1b1-53d9058fce32" class="">因此一般t-SNE的作用比較不是用在這種training testing的這種base上面，而是拿來做visualization
也就是說如果已經有一大堆的x，他是high dimensional的，
那想要visualize他們在二維空間的分佈上是什麼樣子，就可以使用t-SNE。</p><figure id="c77479b2-7803-4ad1-8e42-41e3fedc0a82" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image34.png"><img style="width:575px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image34.png"/></a></figure><h3 id="7fcb2e13-4fec-45b0-8e36-df74c17bd244" class="">1. t-SNE–Similarity Measure</h3><p id="2e61b589-e466-4655-9a0b-81535602de35" class="">下圖橫軸代表了在原來space上的Euclidean distance，
或是做dimension reduction 以後的Euclidean distance；
而紅線是RBF function，藍線是t-distribution。</p><figure id="a7e451c2-4586-41c7-93c2-686f018f5e0f" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image35.png"><img style="width:596px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image35.png"/></a></figure><p id="71f88869-dd00-4d69-9b29-3cb8d3443d5a" class="">原來橘點做dimension reduction以後，為維持他們原來之間的距離，會轉換為藍點。
此時就可得知，變到t-distribution以後，原來在高維空間裡面如果距離很近，
做完transform以後他還是很近；如果原來就已經有一段距離，
那做完transform以後他就會被拉得很遠。</p><h1 id="9b85d2a2-9d4c-4de1-8e9a-879c034e8c53" class="">Auto-encoder</h1><h2 id="0de310e9-3abc-44e3-95e4-6328fe2483c9" class="">一、Deep Auto-encoder</h2><p id="8c4624d3-d803-4d34-bb67-119225f4ac78" class="">假設該做的是MNIST的話，input就是一張digit，他是784維的vector；
output就是一個遠比784維還要小的code。</p><p id="f0ffb6e1-8371-4b2f-a644-110a7b5648fe" class="">也就是我們需要learn一個encoder，讓input通過後變成code。
為了做到這件事必須同時learn一個decoder，使code還原回原本的圖片。</p><p id="a397751c-ef89-498c-bbbc-27c31d1a4bf2" class="">接著就是將他們接起來變成一個neural network，並希望他的input與output距離越近越好。</p><figure id="00fcf475-2951-457a-88b2-d0b19f6cfb66" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image36.png"><img style="width:537px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image36.png"/></a></figure><figure id="6fcf46f3-9fd7-48b3-ae8d-f0b23c18f25d" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image37.png"><img style="width:526px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image37.png"/></a></figure><p id="6c79ae1c-f506-47ca-95b6-cb5f1f45b927" class="">當成neural network來看的話，input的x就是input layer，output的x\head就是output layer，
而中間component的weight就是hidden layer，又稱bottleneck layer，他是個特別窄的layer，
代表的是一組code。</p><p id="4b695006-9d46-43fd-b272-08bb0fa2dba7" class="">從input layer到bottleneck layer就是encoder；
而從bottleneck layer的output到output layer就是decoder。</p><p id="99873ac8-89c6-4564-aeb3-5d7ffc5415e2" class="">其training的方法與neural network的方法相同，就是back propagation。</p><figure id="18b12168-bff7-4005-a4b9-37b7f0a6df09" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image38.png"><img style="width:517px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image38.png"/></a></figure><p id="ac6cf468-dae9-46a0-a87e-906dfd95d62f" class="">另外他還可以learn到把雜訊濾掉這件事，就是de-noising auto-encoder。
也就是加了noise以後，還要reconstruct回原來沒有noise的結果。</p><p id="a341b79c-5284-40e5-8b0e-fc6d395a3d86" class="">與他類似的方法叫做contractive auto-encoder，我們會希望說，在learn這個 code的時候，
我們加上一個constrain，表示當input有變化的時候對這個code 的影響是被minimize的。
也就是說希望說當input 變了（加了noise）以後，對這個code的影響是小的</p><figure id="6946faf2-2796-4201-8b8c-9cc19411e9b9" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image39.png"><img style="width:567px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image39.png"/></a></figure><h2 id="3f604cfb-d0eb-4767-8edc-95e0abf6f132" class="">二、Applications</h2><h3 id="17a21c4f-6fe0-40c8-84a3-8bc1de3bd635" class="">1. Encoder for Text Retrieval</h3><p id="917916e5-d026-4391-afa6-afdfc6f155a8" class="">auto-encoder也可以把他用在文字的搜尋處理上。</p><p id="d0c0fdd8-90a2-463b-af6d-76023aea011d" class="">常見的方法為vector space model。也就是把每一篇文章都表示成空間中的一個vector，
然後把查詢的詞彙也變成空間中的一個點。</p><p id="ec806844-968f-4308-9760-ed0c52d45459" class="">接下來就是計算輸入的查詢詞彙，跟每一篇document之間的inner product或是cosine similarity等，
後者會有normalize的效果，可能會得到比較好的結果。</p><p id="d5d40a9d-f0fa-4475-9883-cd07dda8e5b7" class="">這個模型的表現好壞取決於把一個document變成vector表示的好還是不好。</p><p id="b3302cd5-ab62-40ce-9be5-9da5a774d317" class="">最常用的方法叫做bag-of-word。如果想把他做得更好，可以乘上inverse document frequency，
也就是每一維不只會用詞彙在document出現的次數，還會再乘上一個 weight，
代表那個詞彙的重要性。</p><p id="ceaf6731-f41d-4126-93c6-9a22c821e654" class="">（在每個document出現次數越高，重要性越低，如 &quot; is &quot; ）</p><figure id="d71bbe35-a9c8-4829-ab3a-929164c05f1b" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image40.png"><img style="width:500px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image40.png"/></a></figure><p id="e04598d5-850f-4de9-9d16-a8362af8297f" class="">但是此模型沒辦法考慮任何語意相關的東西，因此可以用auto-encoder讓語意這件事情被考慮進來
舉例來說，learn一個auto-encoder，他的input就是一個document，
透過encoder壓成二維的vector後可以visualize為右圖。</p><p id="8ffd2a69-c484-4755-a338-f7029287bc2e" class="">接著如果要查詢詞彙的話，也同樣通過這個encoder，把他變成一個二維的 vector。
最後看他落在哪個區域，就可以得知與何種類別有關。</p><figure id="cd939f66-6357-49e7-bfec-ce75e2078cb7" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image41.png"><img style="width:574px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image41.png"/></a></figure><h3 id="dce1be8d-857a-453f-8f4a-fe73057251f6" class="">2. Encoder for Similar Image Search</h3><p id="f14aa234-e6ef-4dfc-900d-00e7a4fc0e22" class="">如果只在pixel wise上做比較圖片的相似度的話，通常無法找到好的結果。
此時就可以用deep auto-encoder把每一張image變成一個code，然後在code 上面再去做搜尋。</p><figure id="d3c55766-8eea-45db-8c8f-89274201757e" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image42.png"><img style="width:564px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image42.png"/></a></figure><h3 id="bf61ff49-48f2-4624-9484-a12d95ba140f" class="">3. Auto-encoder for CNN</h3><p id="8a49666f-2487-482a-88b7-bf2879441bd1" class="">那如果是做Auto-encoder改善CNN的表現的話，不只要有個encoder還要有個 decoder。
前者的部分就是做 convolution 再做pooling，而後者理論上應該就是做跟encode相反的事情，
也就是unpooling與deconvolution。</p><p id="8de85bf0-185f-42e5-9b67-eb97f3bba355" class="">最後training的criteria一樣就是讓input和output越接近越好。</p><figure id="4f4a3f7f-8054-4da2-9b8e-0787ffb429ab" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image43.png"><img style="width:429px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image43.png"/></a></figure><h3 id="645705d6-b38a-49f4-a96d-2bc846c3bfc9" class="">(1) Unpooling</h3><p id="cd18a57f-da6c-444b-acda-6960fe4148c4" class="">假設圖片在做pooling的時候，是把一個4*4的matrix裡面的pixel分成四個一組，
接下來從每一組裡面挑一個最大的，使image變成原來的四分之一。</p><p id="e24da52c-79ae-4722-99ce-7b2ca96e8dbb" class="">做unpooling的時候，必須記得做pooling的時候是從哪裡取值，然後把原來比較小的matrix擴大，
也就是將剩餘的部分補0或是複製剛剛取pooling的值。</p><figure id="ea67990e-0a1a-413b-90e4-e2a848a0b2fc" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image44.png"><img style="width:557px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image44.png"/></a></figure><figure id="dcdf7029-cf37-4747-9df1-8e5c8a0a3b2a" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image45.png"><img style="width:246px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image45.png"/></a></figure><figure id="7d808a0b-687d-4e88-86a2-a762e91de97e" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image46.png"><img style="width:700px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image46.png"/></a></figure><h3 id="58c4da1d-9868-4cc5-9460-8428a345b857" class="">(2) Deconvolution</h3><p id="ff207081-8ff6-459b-b462-6a13f8fcc9c9" class="">事實上deconvolution就是convolution。</p><p id="7b067578-75e8-4eec-a4a9-fcd3f0ebd9c9" class="">假設input有5個dimension且filter size為3，
然後就把input的這3個value 分別乘上紅色、藍色與綠色的weight，得到一個 output。</p><p id="1a018c7c-5212-4b5a-8173-8a92e418aae4" class="">再把這個filter shift一格，3個value分別乘上weight得到下一個output。
而deconvolution可想成把一個值變成三個值，
也就是一個值分別乘上紅色綠色藍色的weight 變成三個值，
shift一格後也乘上weight變成三個值再跟前者做疊加。
這件事等同於把input做padding，在旁邊補零。</p><p id="921e8e33-a9a2-43a7-b2e3-c8e87b232bc3" class="">接下來一樣做convolution，把三個input乘上綠色藍色紅色的weight得到一個值，
再shift一格，以此類推。
也就是說框框裡面做的事情是一模一樣的，不同點是在他們的 weight 是相反。</p><figure id="359243f2-6aab-4ce8-8721-c393a25a5197" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image47.png"><img style="width:560px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image47.png"/></a></figure><h3 id="1a83a104-b9e2-45e0-bb6a-5e0d26aabc17" class="">4. Auto-encoder for Pre-training DNN</h3><p id="7ae3d1fd-922d-480a-912a-8388fd21695f" class="">在train一個neural network的時候，可以先用auto-encoder找到一組比較好的 initialization，
該流程就叫做pre-training。</p><p id="c5867824-1777-4ae9-8975-3f64c4e396ef" class="">假設為MNIST的recognition，network的input是784 維，第一個hidden layer是1000維，
第二個hidden layer是1000維，然後500到10維。</p><p id="cd4c4ab6-cd19-40a0-a1a0-c281666aed72" class="">在做pre-train的時候就是先train一個auto-encoder，input 是784 維，
然後中間有個1000維的vector，再把他變回784 維，希望input跟output越接近越好。</p><p id="021ca35e-ccd6-4267-b8f7-d8ffd846cfc3" class="">如此就可得到參數W1。</p><p id="f7ddb4fa-d4f2-450a-90e3-e2ca32bbfe16" class="">接著重複上述步驟得到W2與W3，再給W4一個random的初始值，
再用back propagation去調一遍找參數W，我們稱之為 fine tune。</p><p id="22df7846-13fd-4e60-adc7-21fc89c6b559" class="">然而當code 比dimension 還要大時，auto-encoder可能會直接把input硬背起來再輸出，
因此要在這1000維的output加一個很強的regularization (L1)，
也就是希望這1000維的output是sparse，裡面可能只有某幾維是可以有值的，其他維都必須要是零。</p><p id="f151ae39-144e-4a3d-b30e-5a4b51336e9b" class="">pre-training的好處在於，如果有很多unlabeled data與少量的labeled data，
可以用大量的unlabeled data去把W1，W2，W3先initialize好，
那最後的labeled data就只需要稍微調整weight就好。</p><figure id="a5239826-cf6f-4b0d-8022-f038613e2088" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/Untitled%204.png"><img style="width:769px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/Untitled%204.png"/></a></figure><h3 id="8790c8ec-e89d-4554-b056-d336bc56dceb" class="">5. Decoder for Drawing</h3><p id="0c9f9849-dafa-4dbd-aa5d-122621a0fbf0" class="">理論上可以給decoder input random number，而output希望就是一張圖。</p><p id="d0dfa81c-c265-48f7-8781-c4d29e0dde47" class="">以MNIST為例，把code取出來後可以visualize為左邊這張圖，
接著隨機sample vector丟進decoder，產生的結果就為右圖。</p><figure id="3c32cd6e-35fc-4557-8542-0b98ad9ce5bf" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image51.png"><img style="width:335px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image51.png"/></a></figure><figure id="3fa7d48f-45e2-4590-bf6e-5b318180bb0c" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image52.png"><img style="width:629px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image52.png"/></a></figure><p id="2471f251-48a3-4d4c-b87d-bea511fc50bd" class="">然而sample在紅框其他地方，得到東西看起來就不像是image了，
因此我們希望region裡面都是image。</p><p id="ce096b53-ef8a-45f7-967d-688d197162e0" class="">比較簡單的做法就是在code上面加上L2的regularization，讓所有的code都比較接近零，
接下來就在零附近sample，而sample出來的vector就都可以對應到數字。</p><figure id="f6537042-54a0-471b-a345-7f8a74eebb96" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image53.png"><img style="width:569px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image53.png"/></a></figure><h1 id="b9e57dc4-26c9-4035-9e13-e25f70ede628" class="">Generative Models</h1><p id="477296dd-e682-4643-8d54-fba9501e6157" class="">由machine透過input的內容，自行產生與input同類型的物件。</p><h2 id="8d39afd5-c3e3-42a5-bd15-9e20d50a0204" class="">一、Component-by-component</h2><p id="252aa53e-d865-44d2-96c1-70b2186bf45f" class="">以圖像為例，假設我們先隨機給model第一個input，也就是右圖紅色與藍色的pixel 
（皆表示為RGB共三維的vector），而假設他的output為淺藍色pixel。</p><p id="6483feae-05b7-4adc-b598-259dcc616aa4" class="">接著將該pixel 再擺到input，之後產生灰色pixel。依此類推即可完成一個如左圖的3*3的image。</p><figure id="725eacaf-f424-42b7-8936-49f124cc316b" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image54.png"><img style="width:477px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image54.png"/></a></figure><p id="4537de7e-2fab-4781-bf06-23e5da89b56d" class="">由於該neural network input的pixel length與output的pixel length並不一致，
因此該例通常會使用RNN解決該問題。</p><p id="29ee5cc8-7811-4f3c-92f6-89ebe3d30c03" class="">由上述知，我們可以透過該方法讓machine predict image中遺失的部分，
或是直接將wave form當作input predict下一個sample的結果。</p><p id="ddeac8da-80ec-4a18-8c48-87471eb98763" class="">該方法產生image的優點在於，它產生的image的解析度較高。</p><figure id="164a9e0e-6ca9-413b-990b-92ef30f9db91" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image55.png"><img style="width:517px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image55.png"/></a></figure><h2 id="9d868be8-7642-454a-9bdd-2907c68a6280" class="">二、Variational Auto-encoder (VAE)</h2><p id="991b22e8-e04d-41c6-aa57-2ed63392163a" class="">已知在Auto-encoder的image case中，我們可以將decoder抽出，並藉由input一組vector，
讓decoder可以自行產生image。</p><p id="58d8166c-6b55-47d1-a9b7-e3d4fbee42ba" class="">然而直接使用Auto-encoder得到的performance通常不一定很好，
因此可以使用Variation Autoencoder進一步改良該model。</p><p id="54a780c8-34a8-40b3-a0e0-4a1eda510f99" class="">在VAE中encoder跟decoder的部分維持原狀，但在encoder的地方不是直接output code，
而是output兩個vector。</p><p id="1743790e-4817-41cf-a6d7-629b6211b31e" class="">假設code是三維的話，那output的這兩個vector也都是三維，分別為m1, m2, m3與σ1, σ2, σ3。</p><p id="2c92fcc4-67b8-4508-b65a-60a128d58fd1" class="">然後再用normal distribution去generate另一個也是三維的vector，e1, e2, e3。</p><p id="b834cbb2-baab-42b0-83c9-9c1b4acddec9" class="">接下來把σ1, σ2, σ3取exponential跟e1, e2, e3相乘，然後再把他跟m1, m2, m3加起來，
得到c1, c2, c3，也就是encoder output的code，寫作：</p><figure id="7d6cc057-c830-4a15-a302-f11dc6a6f50d" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>i</mi></msub><mtext> </mtext><mo>=</mo><mtext> </mtext><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>σ</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mtext> </mtext><mo>×</mo><mtext> </mtext><msub><mi>e</mi><mi>i</mi></msub><mtext> </mtext><mo>+</mo><mtext> </mtext><msub><mi>m</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">c_i = \exp(σ_i) × e_i + m_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"> </span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"> </span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">exp</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"> </span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"> </span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"> </span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"> </span><span class="mord"><span class="mord mathdefault">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></div></figure><p id="6014c808-9859-4a05-bd39-970ece6e06be" class="">最後丟到decoder裡面，希望同時minimize reconstruction error與下式：</p><figure id="f03e22f8-b44d-4410-8f26-9be30a43abed" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mn>3</mn></munderover><mo stretchy="false">(</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>σ</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><msub><mi>σ</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><msub><mi>m</mi><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\sum^3_{i=1}(\exp(σ_i)+(1+σ_i)+(m_i)^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.0787820000000004em;vertical-align:-1.277669em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8011130000000004em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mop">exp</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></div></figure><figure id="980ea582-9612-4cf0-9fc0-519e3093c034" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image56.png"><img style="width:622px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image56.png"/></a></figure><p id="8de800ea-5fc7-4325-bd1e-46780d78aba9" class="">VAE中的參數 m，可以理解為原來的 code，而c代表的則是加上noise以後的code。
因此decoder做的是，根據加上noise以後的code把他reconstruct回原來的image。</p><p id="7b7dc621-2101-4260-9486-2ead9ceda676" class="">而σ代表了現在這個noise的variance，因為variance是正的，所以這一邊會取一個exponential，
為確保neural network的output在沒有activation function去控制他的情況下，output皆為正的。</p><p id="4f0396c2-1f7c-4a2d-b75f-7ef2b3c7e764" class="">當把σ乘上e再加到m的時候，就等於是把這個m加上了noise，也就是把原來code加上noise。
而e是從normal distribution sample出來的，所以他的variance是固定的，乘上不同σ以後，
variance 的大小就有所改變。</p><p id="d1d54488-068d-4e4c-8d90-d75167447ac1" class="">所以這一個variance決定了noise的大小。這個variance是從encoder產生的，
也就是說machine在training的時候，會自動去learn這個variance應該要有多大，
然後decoder要reconstruct回原來的image。</p><figure id="fae13599-35df-4a57-97fc-51a34c8e1e24" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image57.png"><img style="width:628px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image57.png"/></a></figure><p id="1bbee0db-e998-48cd-bbe9-b134a2416dd5" class="">為避免machine 決定所有的variance都是0的情況，依此必須加上一些限制，
強迫他的variance不可以太小，也就是黃框內function做的事情。</p><p id="2c4ca5f2-054b-48c3-aeae-28c342094dbd" class="">把藍色底線的function減去紅色底線的function時，得到圖中的綠色曲線。</p><p id="cf7a271c-bc99-452e-9ba5-5c5fa782defc" class="">它的最低點落在σ= 0的地方，由於之後會取exponential的緣故，所以該點的的variance是1，
也就是σ= 0的時候loss 最低。因此machine 就不會讓 variance等於0，
然後minimizes reconstruction error。此外他還要考慮variance是不能夠太小。</p><p id="004cd18b-da93-437f-8404-8812a763c3d1" class="">最後這一項(mi)2就是L2的regularization讓他的結果比較sparse，以至於比較不會overfitting。</p><figure id="80271f95-ad8a-41d1-972d-01840dbc9cc5" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/Untitled%205.png"><img style="width:782px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/Untitled%205.png"/></a></figure><p id="0a80623c-5217-4fe0-b596-5b4a1cf2b309" class="">加上noise好處在於，假設要generate的image為月相的圖片時，
原來的Auto-encoder只有特定的一點可以被reconstruct回滿月的圖；
但對VAE來說，加上上noise後，在某段range的code都可以可以被reconstruct回滿月的圖。</p><p id="6d7eb89c-5c5e-4316-a9e6-1ce470d679c9" class="">接著在下圖兩段range交界的地方，這個code的點同時希望被reconstruct回弦月的圖與滿月的圖。</p><p id="6024e33d-2a6f-4b14-8348-a6c116493a22" class="">而VAE training的時候，minimize 這個mean square error，可以產生一張介於滿月和弦月中間的圖，同時讓他最像滿月也最像弦月。</p><figure id="d8c9a634-8938-468c-b8e1-df89cc196f8c" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/Untitled%206.png"><img style="width:686px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/Untitled%206.png"/></a></figure><h2 id="124e9da4-18bd-4049-8e9a-a97a7b5edd85" class="">三、Generative Adversarial Network (GAN)</h2><p id="b43db3ee-f5e5-426a-805e-d8b1cd79ccfd" class="">VAE從來沒有去真的學怎麼產生一張看起來像真的 image，他所學到的事情是，
產生一張image跟在data base裡面某張image越接近越好。</p><p id="0524f169-1f69-4e1c-9ef2-245b04d7d6bd" class="">但在evaluate 產生的image跟data base裡面的image的相似度時，
會把下圖兩張image的差異度皆定為相同（差一個pixel），
即便我們可以明確分辨出左圖比右圖較為合理。</p><figure id="6ddccdcf-d39f-4062-9f0d-9d8e737c25ba" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image60.png"><img style="width:612px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image60.png"/></a></figure><p id="cabb488b-2ebd-4ea1-a141-45b2517f5f70" class="">所以VAE最多做到的是只是模仿，或者把原來data base裡面的image做linear combination，
沒辦法產生一些新的image。因此就有了GAN的方法出現。</p><p id="dbb907d9-6acd-4093-9138-dd3d991e6b30" class="">GAN的概念有點像似擬態的演化。比如說一個枯葉蝶與天敵的互動關係，
一開始為躲避捕食，枯葉蝶會演化為棕色，使天敵無法分辨他與枯葉的差異。</p><p id="55ad2052-3b54-4cab-a830-769a34247a8b" class="">接著天敵也會慢慢演化為，能辨別枯葉蝶與枯葉差別在於有無葉脈這件事。</p><p id="0a060c10-1627-452f-b286-25d75e31bf01" class="">然後枯葉蝶又為演化出就產生葉脈的擬態，而天敵也同樣演化出識破擬態的方法的。
依此類推，直到天敵最後沒有辦法分辨為止。</p><figure id="b8dc274b-71f2-4eb2-9b03-07e16bff0efe" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image61.png"><img style="width:575px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image61.png"/></a></figure><p id="7c55b809-d190-4454-b69e-a52005386e6e" class="">基於上述理念，GAN 的架構大致大致上為：</p><p id="8aaa9fdb-eb32-45da-9549-29a76ba2a65c" class="">首先會有一個第一代的Generator與Discriminator，前者透過input的code產生image；
而後者則根據第一代Generator output的image與real image，
調整他的參數去評斷一張image是real image還是Generator所產生的image。</p><p id="0933ce93-f43e-4d50-b36f-5261405562f0" class="">接下來呢，這個Generator根據Discriminator分辨的結果，調整參數產生第二代的Generator；</p><p id="55e38d98-95ed-4820-a0c5-38aaec061a5b" class="">而Discriminator會再根據第二代Generator與real image調整參數，
同樣產生第二代的Discriminator，依此類推，直到Discriminator fail為止。</p><p id="170b6ec7-9d5f-4445-9537-c789bf3f306c" class="">也就是說Generator從來沒有看過真正的image長什麼樣子，
他做的事情只是想去騙過Discriminator；</p><p id="620a4752-04ad-44b0-9933-1e70d67c064f" class="">而Discriminator有看過真正的image長什麼樣子，
他會比較真正的image跟Generator output有何不同。</p><figure id="5cfcd8dd-e59a-4f27-a327-7bedd6879ce0" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image62.png"><img style="width:569px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image62.png"/></a></figure><h3 id="6ad4a63f-9ede-4b0d-9061-b21217f4b910" class="">1. Discriminator</h3><p id="4ebd7c2b-05a0-473a-8a56-9c5efc2d249c" class="">Discriminator就是一個neural network，他的input就是一張image；output就是一個number，
假設通過sigmoid function，讓他的值介於0到1之間。</p><p id="3921fe1c-1434-4551-9fdf-67d5d7400889" class="">1代表input這張image是真正的image。
假如是MNIST的話，那input image就是真正的人手寫的數字；
而0就代表是假的，是Generator所產生的。</p><p id="cd99bc1c-cfb3-45a3-833b-0b392de294d3" class="">Generator在這一邊的架構就跟VAE的decoder是一模一樣的，他也是一個 neural network。
input就是從某一個distribution sample出來的一個vector，
而他就會產生一個數字的image。</p><p id="69cc1cd7-d115-47bf-8266-46dc3d25a260" class="">因此Discriminator就是把這一些Generator所產生的image都label為0，
然後把真正的image都label為1，為binary classification的problem。</p><figure id="531a2871-8c0b-44a8-b94f-b2d3ee2fcc57" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image63.png"><img style="width:547px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image63.png"/></a></figure><h3 id="0facbdaa-e49d-43d5-9cf1-f0776f5fa150" class="">2. Generator</h3><p id="6c8f0441-df4f-4ff6-b1d7-de1255d67386" class="">首先如果我們隨便輸入一個vector，Generator會產生一張隨便的image，
那這一張image可能沒有辦法騙過Discriminator。</p><p id="a476467a-3dcb-4f04-8a4f-0a440c6fc59d" class="">然後接下來就要調Generator的參數，
讓Discriminator會認為Generator generate出來的image是真的，
也就是說讓Generator generate出來的image，丟到Discriminator以後，
Discriminator的output必須要越接近1越好。</p><p id="6aca4642-34aa-4e4a-bc22-0adc9a62f971" class="">此處可以把這個Generator的output丟到Discriminator的input，然後再讓他產生一個scalar。
也就是把Generator與Discriminator合起來，當作一個很大的neural network，
input為random vector；output就是一個scalar。</p><p id="d67b3172-c585-46e7-bf6a-e17d37666086" class="">接著我們希望丟進這一個vector的時候，他的output是接近1的。</p><p id="2d30691d-ad4e-45d3-810b-9f8ee230421a" class="">但是這邊要注意的事情是，在做Backpropagation調參數的時候，只能夠調整Generator的參數。
也就是fix住Discriminator的參數，只能算generator的參數對output的gradient，
然後去update Generator的參數。</p><figure id="e6ba9a98-817f-4644-b7db-9c640194a407" class="image"><a href="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image64.png"><img style="width:557px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Unsupervised%20Learning/image64.png"/></a></figure></div></article></body></html>