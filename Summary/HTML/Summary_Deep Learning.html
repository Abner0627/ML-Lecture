<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Summary_Deep Learning</title><style>
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	text-indent: -1.7em;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.highlight-default {
}
.highlight-gray {
	color: rgb(155,154,151);
}
.highlight-brown {
	color: rgb(100,71,58);
}
.highlight-orange {
	color: rgb(217,115,13);
}
.highlight-yellow {
	color: rgb(223,171,1);
}
.highlight-teal {
	color: rgb(15,123,108);
}
.highlight-blue {
	color: rgb(11,110,153);
}
.highlight-purple {
	color: rgb(105,64,165);
}
.highlight-pink {
	color: rgb(173,26,114);
}
.highlight-red {
	color: rgb(224,62,62);
}
.highlight-gray_background {
	background: rgb(235,236,237);
}
.highlight-brown_background {
	background: rgb(233,229,227);
}
.highlight-orange_background {
	background: rgb(250,235,221);
}
.highlight-yellow_background {
	background: rgb(251,243,219);
}
.highlight-teal_background {
	background: rgb(221,237,234);
}
.highlight-blue_background {
	background: rgb(221,235,241);
}
.highlight-purple_background {
	background: rgb(234,228,242);
}
.highlight-pink_background {
	background: rgb(244,223,235);
}
.highlight-red_background {
	background: rgb(251,228,228);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(55, 53, 47, 0.6);
	fill: rgba(55, 53, 47, 0.6);
}
.block-color-brown {
	color: rgb(100,71,58);
	fill: rgb(100,71,58);
}
.block-color-orange {
	color: rgb(217,115,13);
	fill: rgb(217,115,13);
}
.block-color-yellow {
	color: rgb(223,171,1);
	fill: rgb(223,171,1);
}
.block-color-teal {
	color: rgb(15,123,108);
	fill: rgb(15,123,108);
}
.block-color-blue {
	color: rgb(11,110,153);
	fill: rgb(11,110,153);
}
.block-color-purple {
	color: rgb(105,64,165);
	fill: rgb(105,64,165);
}
.block-color-pink {
	color: rgb(173,26,114);
	fill: rgb(173,26,114);
}
.block-color-red {
	color: rgb(224,62,62);
	fill: rgb(224,62,62);
}
.block-color-gray_background {
	background: rgb(235,236,237);
}
.block-color-brown_background {
	background: rgb(233,229,227);
}
.block-color-orange_background {
	background: rgb(250,235,221);
}
.block-color-yellow_background {
	background: rgb(251,243,219);
}
.block-color-teal_background {
	background: rgb(221,237,234);
}
.block-color-blue_background {
	background: rgb(221,235,241);
}
.block-color-purple_background {
	background: rgb(234,228,242);
}
.block-color-pink_background {
	background: rgb(244,223,235);
}
.block-color-red_background {
	background: rgb(251,228,228);
}
.select-value-color-default { background-color: rgba(206,205,202,0.5); }
.select-value-color-gray { background-color: rgba(155,154,151, 0.4); }
.select-value-color-brown { background-color: rgba(140,46,0,0.2); }
.select-value-color-orange { background-color: rgba(245,93,0,0.2); }
.select-value-color-yellow { background-color: rgba(233,168,0,0.2); }
.select-value-color-green { background-color: rgba(0,135,107,0.2); }
.select-value-color-blue { background-color: rgba(0,120,223,0.2); }
.select-value-color-purple { background-color: rgba(103,36,222,0.2); }
.select-value-color-pink { background-color: rgba(221,0,129,0.2); }
.select-value-color-red { background-color: rgba(255,0,26,0.2); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="7822fdad-8e50-4a2f-a68a-5f1978453418" class="page sans"><header><h1 class="page-title">Summary_Deep Learning</h1></header><div class="page-body"><nav id="a2105a8f-aabd-44ad-ac9f-6d52f5e84d08" class="block-color-gray table_of_contents"><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#c36921ba-a836-4f53-a584-f3b5cb79b845">DN的三個步驟</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#53978ea9-2afc-4fd4-a362-51d025a8dd4e">一、限定Model的範圍</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#ee4e58b0-043f-453d-b78f-0ed1234bd9de">1. Matrix Operation</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#5af9ff90-23dc-421e-9d9e-fe9d4b9b89ff">2. Feature Extractor</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#b793e0e6-8dfe-44c2-bfd8-4866188c5dd6">3. Multi-class Classifier</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#49fcb408-89f1-4936-b20c-1fbf30f0ca33">二、從範圍內找出好的function</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#b6cbfa47-5b57-4afa-8c77-e120a0c9182b">三、找出最小的Loss Function</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#c734b681-043c-4c54-ac9e-18b84a25b5d3">1. Backpropagation</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#a53920e6-23a1-4fa1-99e6-6ca67dcb0241">(1) Forward Pass</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#7ab9b4a7-4ee6-46f2-964a-b48b9abf3d5c">(2) Backward Pass</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#f9d7430b-4f42-4c3e-98c3-a183e55766d6">Case I Output Layer</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#08ce0662-a7be-4d61-9f35-3c313d4d185e">Case II Not Output Layer</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#5c02d4b0-9de9-4951-9be5-1b45d5b1c4d0">Summary</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#64446056-b34e-4429-b049-6a579c5d3dfb">四、Tips for Deep Learning</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#d266d0af-06ee-4bba-991c-4af3f58b95ca">1. <strong>Performance in Training Data</strong></a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#797a03bc-a658-4f62-a308-a11e5a281352">(1) New Activation Function</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#3d87c03b-73fd-4d4b-a9e7-7fa418a6b8eb">a. ReLU</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#22a4cf3b-29b5-4357-a554-0b1fd637dd39">b. Leaky ReLU與Parametric ReLU</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#4bc16bfb-8054-4930-b121-055699f69490">c. Maxout</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#5a62d778-5019-46b3-9fd3-f4e99bb1cf0d">(2) Adaptive Learning Rate</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#3a5ca134-059b-409a-91e3-613223767b01">a. RMSProp</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#a3ca0c39-35e4-4e76-b1be-eb4b09f8e679">b. Momentum</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#ed03dff3-a071-4619-b45e-e3e69f92ed39">c. Adam</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#c80da7fd-d9e3-4f69-8d66-8dd1fe79c922">2. Performance in Testing Data</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#1afce9b4-7533-4006-b9ed-bf70f878391a">(1) Early Stopping</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#89368578-448c-4960-b93a-914386b30dfc">(2) Regularization</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#75c78857-2efb-4cd9-9b4c-6134471b5f9f">L1</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#1799f818-7f26-4f18-ab9e-186d8ce7f737">L2</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#d3847979-ab9b-4f8a-9953-e5317f146dc6">L1 vs. L2</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#6daea7ff-b862-413e-aa5c-99b41bf2466e">(3) Dropout</a></div></nav><h1 id="c36921ba-a836-4f53-a584-f3b5cb79b845" class="">DN的三個步驟</h1><h2 id="53978ea9-2afc-4fd4-a362-51d025a8dd4e" class="">一、限定Model的範圍</h2><p id="0ecde2d1-6566-476e-948d-b28bb4c050fd" class="">在使用DL時，實際上就是選擇 Neural Network的model進行運用。</p><p id="8eaffa0e-30cb-44b9-a2fc-d4f6e0193dbb" class="">而依照neural network的類型又細分成一般常見疊很多層layer的Deep Neural Network以及特化用於影像分析類別的Convolutional Neural Network。</p><p id="f8c20799-6c7e-4758-8e42-7847494ca7a3" class="">可以將neural network想成由數個Logistic Regression前後connect在一起，然後把一個Logistic Regression稱之為neuron。</p><p id="2b6c69bf-0524-4bf7-9fcd-40ecca85d592" class="">每個Logistic Regression都有自己的weight和bias，這些weight和bias集合起來，就是這個network的parameter，我們用θ表示。</p><h3 id="ee4e58b0-043f-453d-b78f-0ed1234bd9de" class="">1. Matrix Operation</h3><p id="5c0602c0-1e2b-4b8b-aafd-d91f9454fb1e" class="">Network的運作過程我們可以使用matrix的方式來表示，每一個row對應的是一個neuron的weight，row的數量就是neuron的個數；而input x，bias b和output y都是一個行向量，row的數量就是feature的個數。</p><figure id="58ffeb95-e475-4a08-b192-cbd7c7517437" class="image"><img style="width:590px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Deep Learning/image1.png"/></a></figure><p id="81f8f4e5-845b-434f-a414-9008a732f988" class="">該方法使得DNN的運算得以透過矩陣運算完成，因此可使用GPU加速。</p><h3 id="5af9ff90-23dc-421e-9d9e-fe9d4b9b89ff" class="">2. Feature Extractor</h3><p id="93b73f0f-e621-406f-9a16-34b3dfd48714" class="">我們可以把hidden layers這部分，看作是一個feature extractor，該處取代了做ML時手動進行feature engineering、feature transformation這些事情。</p><p id="4ecb03a2-027e-4f4c-a27a-6f3de4c4cbad" class="">因此DL將原本ML從本來如何抽取feature的問題轉化成怎麼design network structure。</p><h3 id="b793e0e6-8dfe-44c2-bfd8-4866188c5dd6" class="">3. Multi-class Classifier</h3><p id="918fff3b-62ab-4dd8-8160-4751c9190bd4" class="">而Output layer做的事情，其實就是把它當作一個Multi-class classifier。他會利用feature extractor分類出的feature對input進行分類，我們會在最後一個layer加上softmax用來預測每個class可能的機率。</p><figure id="ca5d9dd8-08bc-4d5e-bada-698c7cb6225b" class="image"><https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Deep Learning/image2.png"/></a></figure><h2 id="49fcb408-89f1-4936-b20c-1fbf30f0ca33" class="">二、從範圍內找出好的function</h2><p id="f9690964-1f8a-4fe6-b4d3-e019a5804ee2" class="">以Multi-class classification為例，我們可將softmax預測的結果與true label得到的target進行cross entropy（同ML的classification的問題）。</p><figure id="dc62b805-be40-4d73-bb92-35f81f1e979d" class="image"><img style="width:572px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Deep Learning/image3.png"/></a></figure><figure id="85d57798-a65f-44f8-8432-584d4975efd1" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><mi>n</mi></munder><mi>l</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><msubsup><mi>x</mi><mi>i</mi><mi>n</mi></msubsup><mo stretchy="false">)</mo><mo separator="true">,</mo><msup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>n</mi></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L(f)=\sum_n l(f(x_i^n), \hat{y}^n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">L</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.3000100000000003em;vertical-align:-1.250005em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8999949999999999em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.250005em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7143919999999999em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7143919999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></div></figure><figure id="a8ee19ea-6cb1-4a72-8c12-e899dba8671a" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><msup><mi>x</mi><mi>n</mi></msup><mo stretchy="false">)</mo><mo separator="true">,</mo><msup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>n</mi></msup><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mo stretchy="false">[</mo><msup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>n</mi></msup><mi>ln</mi><mo>⁡</mo><mrow><mi>f</mi><mo stretchy="false">(</mo><msup><mi>x</mi><mi>n</mi></msup><mo stretchy="false">)</mo></mrow><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>n</mi></msup><mo stretchy="false">)</mo><mi>ln</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>f</mi><mo stretchy="false">(</mo><msup><mi>x</mi><mi>n</mi></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">l(f(x^n),\hat{y}^n)=-[\hat{y}^n\ln{f(x^n)}+(1-\hat{y}^n)\ln(1-f(x^n))]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7143919999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7143919999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">−</span><span class="mopen">[</span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7143919999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">ln</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7143919999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span></span></span></span><span class="mclose">)</span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7143919999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">ln</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7143919999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span><span class="mclose">]</span></span></span></span></span></div></figure><h2 id="b6cbfa47-5b57-4afa-8c77-e120a0c9182b" class="">三、找出最小的Loss Function</h2><p id="47df1976-0b4e-4ae8-b47b-162ba74fe1ab" class="">將所有data的cross entropy都加總起來就會得到total loss，得到loss function之後找一組network的parameters（θ*）使total loss最小，此處使用gradient descent即可（方法同linear regression）。</p><figure id="e73b90e2-b11f-4847-b765-a636a01731b9" class="image"><img style="width:583px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Deep Learning/image4.png"/></a></figure><figure id="eda4b981-c1ae-4b07-a03c-dc9cee137f29" class="image"><img style="width:681px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Deep Learning/image5.png"/></a></figure><h3 id="c734b681-043c-4c54-ac9e-18b84a25b5d3" class="">1. Backpropagation</h3><p id="5581392d-ffe0-4e8d-8883-ae7f98c4db0e" class="">由於NN的parameters非常多，無法很有效率地一個個取其gradient，因此可用Backpropagation增加運算效率。</p><p id="ec947e23-3a61-4ca2-88bc-3d10fb66e172" class="">因為loss function為所有training data的cross entropy的總和，因此只要考慮某一筆data的cross entropy對參數w的偏微分再加總，就可以把total loss對某一個參數w的偏微分給計算出來。</p><p id="e0adde64-6e0d-4a1e-94a2-68611c37c794" class="">先考慮某一個假設只有兩個input的neuron經過activation function從這個neuron中output出來，接著再經過數個neuron得到最終的output：y1, y2。</p><p id="7c53a5b0-90b8-46ea-995c-cc4909092f9c" class="">而該他們與target的cross entropy對w的偏微分可分作Forward pass與Backward pass討論。</p><figure id="f3244269-5c69-4e72-89f2-81e8d7f70e3e" class="image"><img style="width:575px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Deep Learning/image6.png"/></a></figure><h3 id="a53920e6-23a1-4fa1-99e6-6ca67dcb0241" class="">(1) Forward Pass</h3><p id="2d4f770b-9cbf-40d9-b66e-670ee3ce4a47" class="">即∂z/∂w，該項可透過上圖得知∂z/∂w1與∂z/∂w2分別為<em>x</em>1與<em>x</em>2。故知w前面連接的input是什麼，那微分後的值就是什麼。</p><figure id="4b1f75f3-02dd-4e78-93f8-660b8a6a2acf" class="image"><img style="width:571px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Deep Learning/image7.png"/></a></figure><h3 id="7ab9b4a7-4ee6-46f2-964a-b48b9abf3d5c" class="">(2) Backward Pass</h3><p id="05e2c3a3-b02d-41dd-8845-82d2a08f7555" class="">此處假設activation function是sigmoid function，透過chain rule可將∂l/∂z不斷轉換直到下圖最後的式子，此時只要得知∂l/∂z&#x27;與∂l/∂z&#x27;&#x27;即可。</p><figure id="7f3d9a3f-56cd-47eb-be1d-84bb6d6d77a9" class="image"><img style="width:579px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Deep Learning/image8.png"/></a></figure><p id="d0d1b24b-6a3c-412a-acce-7c4b99f5cf07" class="">而∂l/∂z&#x27;與∂l/∂z&#x27;&#x27;又可分作兩個case討論：</p><h3 id="f9d7430b-4f42-4c3e-98c3-a183e55766d6" class="">Case I Output Layer</h3><figure id="749a5675-a912-47cc-be2f-de611327283c" class="image"><img style="width:587px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Deep Learning/image9.png"/></a></figure><p id="bfae1c65-143b-41c9-ae55-50e301a39687" class="">∂y1/∂z&#x27;與∂y2/∂z&#x27;&#x27;只要知道前面紅色的activation function就可以得出其值。</p><p id="4546d58b-24a3-4a42-80ef-720e91f0b978" class="">∂l/∂y1與∂l/∂y2取決於loss function是怎麼定義的（例如cross entropy）。</p><h3 id="08ce0662-a7be-4d61-9f35-3c313d4d185e" class="">Case II Not Output Layer</h3><p id="4c22db40-8455-46f9-a506-1e2e6f66ac17" class="">這裡我們可以另一個觀點來看待這個式子，想像一個新的neuron為下列形式。</p><figure id="c395c1b5-c23a-47de-a3ba-b7b7f342920d" class="image"><img style="width:589px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Deep Learning/image10.png"/></a></figure><p id="16a43b67-6c19-4fa5-bf61-cedb1f537034" class="">將其進一步推廣至原本的network中，反覆運算下去直到抵達output layer便可適用Case I。</p><figure id="17c08baa-9cbf-4d61-b4bd-82188e34fe51" class="image"><img style="width:585px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Deep Learning/image11.png"/></a></figure><figure id="b6a6f2d3-ad96-4869-b866-0214efa14f07" class="image"><img style="width:700px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Deep Learning/image12.png"/></a></figure><h3 id="5c02d4b0-9de9-4951-9be5-1b45d5b1c4d0" class="">Summary</h3><p id="258a1075-5ff8-4758-acd1-3f9c0e9d8b3d" class="">Forward pass：每個neuron的activation function的output，就是它所連接的weight的∂z/∂w。</p><p id="9264dc56-7392-40b1-8f55-655bbfbcfd45" class="">Backward pass，建一個與原來方向相反的neural network，它的三角形neuron的output就是∂l/∂z。</p><p id="d67e8474-f79f-4e67-89fb-16cc35160010" class="">兩者相乘即為所求：</p><figure id="ec88b4ba-7d2f-47d3-bc38-c611e1e48190" class="image"><img style="width:432px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Deep Learning/image13.png"/></a></figure><figure id="802da66d-c62d-4fd0-a6a2-0c19610a1a32" class="image"><img style="width:551px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Deep Learning/image14.png"/></a></figure><h2 id="64446056-b34e-4429-b049-6a579c5d3dfb" class="">四、Tips for Deep Learning</h2><p id="c128c2a8-7f00-46a0-ac42-6890c2083269" class="">再結束上述三個步驟後，為使NN的表現更好，我們通常會再檢查model在training data與testing data的表現。</p><figure id="989f83cb-64d4-4dd1-9b3a-125216e946b4" class="image"><img style="width:700px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Deep Learning/image15.png"/></a></figure><h3 id="d266d0af-06ee-4bba-991c-4af3f58b95ca" class="">1. <strong>Performance in Training Data</strong></h3><p id="6f3eb0f2-8e8e-4749-9119-26def713da37" class="">此處為DL一項特別的地方，若非DL方法（如k-nearest neighbor或decision tree）其在training set上的performance正確率就是100，並無檢查必要。也因如此非DL方法非常容易overfitting，而對DL來說overfitting往往不會是遇到的第一個問題。以下有兩個方式可以增加其在在training set上的performance。</p><h3 id="797a03bc-a658-4f62-a308-a11e5a281352" class="">(1) New Activation Function</h3><p id="31277c1b-d26a-4ae7-a292-1b9ec2bcf8fe" class="">使用sigmoid function時network疊得很深的時候準確率反而會下降，該處稱Vanishing Gradient Problem。</p><p id="782858a2-da87-4af6-95ba-9b653b9a0e6e" class="">意即在靠近input的地方，這些參數的gradient（對loss function的微分）是比較小的；而在比較靠近output的地方，它對loss的微分值會是比較大的。</p><p id="ca9b72ef-335f-4a5e-a247-81bfeb085969" class="">因此設定同樣learning rate的時候，靠近input的地方，它參數的update是很慢的；而靠近output的地方，它參數的update是比較快的。</p><figure id="98aeb1e5-e8a0-4f51-bc38-67de1e3f454a" class="image"><img style="width:579px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Deep Learning/image16.png"/></a></figure><p id="ac4d90c8-d5c9-4aec-be25-3dfe34a63e76" class="">原因在於sigmoid function會不斷縮小參數的變化，導致input對loss的影響會比較小，於是靠近input的那些weight對loss的gradient 遠小於靠近output的gradient。</p><figure id="4a1a9927-efc1-4b40-b5ff-e5627a568944" class="image"><img style="width:700px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Deep Learning/image17.png"/></a></figure><p id="59e01e6b-d214-4021-a4ef-0e12781457ad" class="">因此改用其他的activation function就可以解決該問題。</p><h3 id="3d87c03b-73fd-4d4b-a9e7-7fa418a6b8eb" class="">a. ReLU</h3><figure id="083a9c5c-69c4-476f-a603-06e8654843d1" class="image"><img style="width:277px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Deep Learning/image18.png"/></a></figure><p id="066ce608-1e51-44b7-8a38-36d9fbd9e8e3" class="">與sigmoid function相比他有下列好處：</p><ul id="8979ef1d-97b8-435f-9719-91ed5c296e0d" class="bulleted-list"><li>可以處理Vanishing gradient的問題。</li></ul><ul id="f56983c9-f30e-4fbf-a301-486aec71ad88" class="bulleted-list"><li>ReLU的想法結合了生物上的觀察（Pengel的paper )。</li></ul><ul id="0d5bd313-1657-4803-bd1b-91e397fbebd9" class="bulleted-list"><li>運算速度較快。</li></ul><ul id="0b65d76b-69c0-498b-ab74-dda9b2415ec9" class="bulleted-list"><li>無窮多bias不同的sigmoid function疊加的結果會變成ReLU。</li></ul><h3 id="22a4cf3b-29b5-4357-a554-0b1fd637dd39" class="">b. Leaky ReLU與Parametric ReLU</h3><p id="688861ee-bda0-4761-9d1c-eecada817a26" class="">使用ReLU時當input&lt;0，output=0，此時微分值gradient也為0，如此便無法更新參數，故提出了其他形式的ReLU解決該問題。</p><figure id="cfb3b4a5-9344-4d8d-a314-1b40aefd3390" class="image"><img style="width:642px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Deep Learning/image19.png"/></a></figure><h3 id="4bc16bfb-8054-4930-b121-055699f69490" class="">c. Maxout</h3><p id="b78a4ed4-d23b-4d40-b2a8-3c05dbe46da0" class="">Maxout的想法是，讓network自動去學習它的activation function，因此一切都是由training data來決定的。</p><p id="ba00457a-96ce-4fbc-a538-d6666e0c885f" class="">該方法將某幾個“neuron”的input分為一個group，然後在這個group裡選取一個最大值作為output。</p><figure id="b1325bd2-29ee-4f8e-8a22-562f2d837e31" class="image"><img style="width:700px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Deep Learning/image20.png"/></a></figure><p id="194f37ca-b5dc-4f0c-9b62-918a0d715486" class="">在具體的實踐上，我們先根據data把max函數轉化為某個具體的函數，意即取走沒有用到的weight，再對這個轉化後的linear network進行微分。</p><p id="885909d2-00ac-4ca8-a132-98a535cd8b6e" class="">因此當你input不同data的時候，得到的network structure是不同的，留在network裡面的參數也是不同的。</p><figure id="ea7db2e8-123d-46f6-a753-e730d9635f84" class="image"><img style="width:689px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Deep Learning/image21.png"/></a></figure><figure id="51ba633f-d898-49dc-be4d-ae79dcd4619a" class="image"><img style="width:693px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Deep Learning/image22.png"/></a></figure><h3 id="5a62d778-5019-46b3-9fd3-f4e99bb1cf0d" class="">(2) Adaptive Learning Rate</h3><p id="d8afda36-4399-4642-98ae-9fd244cba369" class="">在做deep learning的時候，這個loss function可視化之後可以是任何形狀，不便於尋找optimal，因此出現下列兩項方式改善該問題。</p><h3 id="3a5ca134-059b-409a-91e3-613223767b01" class="">a. RMSProp</h3><figure id="60c53004-beb2-4864-8de8-04cfe9a8158f" class="image"><img style="width:370px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Deep Learning/image23.png"/></a></figure><p id="7218cd71-01f4-4fb1-b7a4-df43e6a840c7" class="">Adagrad的進階版，跟Adagrad不同之處在於，Adagrad的分母是對過程中所有的gradient取平方和開根號，也就是說Adagrad考慮的是整個過程平均的gradient訊息；RMSProp雖然也是對所有的gradient進行平方和開根號，但是它用一個α來調整對不同gradient的使用程度。</p><blockquote id="9527ca1e-201a-42f3-833a-dddf013f6bb2" class="">優：可透過調整α改變傾向相信新的gradient所告訴你的error surface的平滑
　　或陡峭程度或舊的gradient所提供的information。</blockquote><h3 id="a3ca0c39-35e4-4e76-b1be-eb4b09f8e679" class="">b. Momentum</h3><p id="fef1a7b2-f550-44a1-a294-deb227631ddd" class="">在做deep learning的時候，由於error surface極為複雜，因此有卡在local minimum、saddle point或是plateau的疑慮。此處便可利用慣性的概念去改良。</p><figure id="54b29e56-15d0-45e2-bb4d-db6aa7629f07" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>θ</mi><mrow><mi>t</mi><mtext> </mtext><mo>+</mo><mtext> </mtext><mn>1</mn></mrow></msup><mtext> </mtext><mo>=</mo><mtext> </mtext><msup><mi>θ</mi><mi>t</mi></msup><mtext> </mtext><mo>+</mo><mtext> </mtext><msup><mi>v</mi><mrow><mi>t</mi><mtext> </mtext><mo>+</mo><mtext> </mtext><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">θ^{t + 1} = θ^{t} + v^{t + 1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.864108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.864108em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mord mtight"> </span><span class="mbin mtight">+</span><span class="mord mtight"> </span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mord"> </span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.926886em;vertical-align:-0.08333em;"></span><span class="mord"> </span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.843556em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span></span></span></span></span></span></span></span></span><span class="mord"> </span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.864108em;vertical-align:0em;"></span><span class="mord"> </span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.864108em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mord mtight"> </span><span class="mbin mtight">+</span><span class="mord mtight"> </span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span></span></div></figure><figure id="73b2b830-ea3a-42f3-807d-693cdf6e5145" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>v</mi><mrow><mi>t</mi><mtext> </mtext><mo>+</mo><mtext> </mtext><mn>1</mn></mrow></msup><mtext> </mtext><mo>=</mo><mtext> </mtext><mi>λ</mi><msup><mi>v</mi><mi>t</mi></msup><mtext> − </mtext><mi>η</mi><mi mathvariant="normal">∇</mi><mi>L</mi><mo stretchy="false">(</mo><msup><mi>θ</mi><mi>t</mi></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">v^{t + 1} = λv^{t} − η∇L(θ^t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.864108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.864108em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mord mtight"> </span><span class="mbin mtight">+</span><span class="mord mtight"> </span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mord"> </span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.093556em;vertical-align:-0.25em;"></span><span class="mord"> </span><span class="mord mathdefault">λ</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.843556em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span></span></span></span></span></span></span></span></span><span class="mord"> </span><span class="mord">−</span><span class="mord"> </span><span class="mord mathdefault" style="margin-right:0.03588em;">η</span><span class="mord">∇</span><span class="mord mathdefault">L</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.843556em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></div></figure><p id="4d05dda2-4caa-40da-999f-29ffa0e8cd78" class="">在gradient descent裡加上Momentum的時候，每一次update的方向，不再只考慮gradient的方向，還要考慮上一次update的方向，那這裡我們就用一個變量v去記錄前一個時間點update的方向。</p><figure id="bcd437e4-bdc3-46a0-b664-07e5868fae1c" class="image"><img style="width:696px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Deep Learning/image24.png"/></a></figure><h3 id="ed03dff3-a071-4619-b45e-e3e69f92ed39" class="">c. Adam</h3><p id="696f57c7-2258-45d3-b979-c34168964b11" class="">綜合RMSProp與Momentum的方法。</p><figure id="468e8d86-8f0a-40fc-b1ac-1ccca5940598" class="image"><img style="width:700px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Deep Learning/image25.png"/></a></figure><h3 id="c80da7fd-d9e3-4f69-8d66-8dd1fe79c922" class="">2. Performance in Testing Data</h3><p id="69e0257e-57d7-44d7-89e1-6a8e67013c04" class="">意即解決overfitting的問題。</p><h3 id="1afce9b4-7533-4006-b9ed-bf70f878391a" class="">(1) Early Stopping</h3><p id="77aebfdd-c667-4793-aba6-4c369a6fba46" class="">理想上假如知道testing data上的loss變化情況，在testing set的loss最小的時候停下來，而不是在training set的loss最小的時候停下來時，就可讓testing set的表現變好。</p><p id="26b698f2-6f99-41e1-a0d1-1e61f722de0c" class="">但testing set實際上是未知的東西，所以我們需要用validation set來替代它去做這件事情。</p><figure id="71bc9b77-f677-47d0-b0fc-9cfe814c6a60" class="image"><img style="width:598px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Deep Learning/image26.png"/></a></figure><h3 id="89368578-448c-4960-b93a-914386b30dfc" class="">(2) Regularization</h3><p id="1e8f879e-3083-4173-bc04-944f6a1ed77b" class="">在原來的loss function上額外增加幾個term使得function更平滑，新加入的term可分作L1與L2兩種。</p><h3 id="75c78857-2efb-4cd9-9b4c-6134471b5f9f" class="">L1</h3><figure id="4c1dcc00-2392-4007-a305-d1935e8fd324" class="image"><img style="width:700px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Deep Learning/image27.png"/></a></figure><p id="d4cbe7db-ddb8-454e-b5f0-27a862e5773b" class="">每次update的時候，都要減去<em>η</em><em>λ</em><em>s</em><em>g</em><em>n</em>(<em>w</em><em>t</em>)，使參數的絕對值減小至接近於0。</p><h3 id="1799f818-7f26-4f18-ab9e-186d8ce7f737" class="">L2</h3><figure id="f1622c81-09cd-4709-9fc5-e8e64808ae47" class="image"><img style="width:700px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Deep Learning/image28.png"/></a></figure><p id="8e8fbf7e-d934-44b4-8cbb-42caa5286cbc" class="">參數在每次update之前，都會乘上一個(1 − ηλ)，而<em>η</em>與<em>λ</em>通常會假設的很小，所以(1 − ηλ)是一個接近於1的值（例如0.99），這意味著，隨著update次數增加，參數會越來越接近於0。</p><p id="88aa81ec-56e2-471e-8079-6b79f38cf8ba" class="">使用L2 regularization可以讓weight每次都變得更小一點，這就叫做Weight Decay。</p><h3 id="d3847979-ab9b-4f8a-9953-e5317f146dc6" class="">L1 vs. L2</h3><p id="3e8fc6da-875c-4433-a657-44ea3005beac" class="">L1和L2，雖然它們同樣是讓參數的絕對值變小，但它們做的事情其實略有不同：</p><ul id="6ab7943c-3bd9-4857-8125-fcb4114b4725" class="bulleted-list"><li>L1使參數絕對值變小的方式是每次update 減掉一個固定的值</li></ul><ul id="cd52890e-3e1f-48e3-97b6-25635942ed3c" class="bulleted-list"><li>L2使參數絕對值變小的方式是每次update 乘上一個小於1的固定值</li></ul><p id="d66dcc42-e303-4a8c-ac1b-6983fd7015e8" class="">當參數w的絕對值比較大的時候，L2會讓w下降得更快，而L1每次update只讓w減去一個固定的值，train完以後可能還會有很多比較大的參數。</p><p id="d309b341-44ab-497b-a91c-14db9931f05e" class="">當參數w的絕對值比較小的時候，L2的下降速度就會變得很慢，train出來的參數平均都是比較小的，而L1每次下降一個固定的value，train出來的參數是比較sparse的，這些參數有很多是接近0的值，也會有很大的值。</p><h3 id="6daea7ff-b862-413e-aa5c-99b41bf2466e" class="">(3) Dropout</h3><p id="d6acf117-8520-4cdb-8857-91c593bb0365" class="">在training的時候，每次update參數之前，我們對每一個neuron（也包括input layer的“neuron”）做抽樣，每個neuron都有p%的機率會被丟掉，如果某個neuron被丟掉的話，跟它相連的weight也都要被丟掉。</p><p id="7c8180ec-7f04-4c14-829f-300bfd0ab905" class="">意即每次update參數之前都通過抽樣只保留network中的一部分neuron來做訓練。</p><p id="beccd292-e811-4d04-b653-3ba84ac835c6" class="">而在做testing的時候，每個neuron都要用到，但所有的weight都要乘上(1-p%)才能被當做testing的weight使用。</p><figure id="e673e6a4-88f3-4a47-8a8d-734155b3b6d2" class="image"><img style="width:667px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Deep Learning/image29.png"/></a></figure><figure id="03a287bc-74af-4114-95af-4feac006a1af" class="image"><img style="width:638px" src="https://abner0627.github.io/ML-Lecture/Summary/HTML/Img/Summary_Deep Learning/image30.png"/></a></figure><p id="434ecb20-7f2d-4210-b76f-76f584f0932e" class="">綜上所述，Dropout會讓training set上的結果變差，但是在testing set上的結果是變好的。（因為某些neuron在training的時候莫名其妙就會消失不見）</p></div></article></body></html>